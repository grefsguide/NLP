{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "q0cjBQohpQDQ",
   "metadata": {
    "id": "q0cjBQohpQDQ"
   },
   "source": [
    "# –°–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Qwen\n",
    "\n",
    "## –û–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞–Ω–∏—è\n",
    "\n",
    "–í —ç—Ç–æ–º –¥–æ–º–∞—à–Ω–µ–º –∑–∞–¥–∞–Ω–∏–∏ –º—ã –Ω–µ –±—É–¥–µ–º –∑–∞–Ω–∏–º–∞—Ç—å—Å—è –æ–±—É—á–µ–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π —Å –Ω—É–ª—è. –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –º—ã —Å—Ñ–æ–∫—É—Å–∏—Ä—É–µ–º—Å—è –Ω–∞ **Inference Engineering** ‚Äî –æ–±–ª–∞—Å—Ç–∏, –∫–æ—Ç–æ—Ä–∞—è –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ —Å—Ç—ã–∫–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –∏ –∑–∞–Ω–∏–º–∞–µ—Ç—Å—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –∏ —É—Å–∫–æ—Ä–µ–Ω–∏–µ–º —Ä–∞–±–æ—Ç—ã —É–∂–µ –æ–±—É—á–µ–Ω–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM).\n",
    "\n",
    "–ú—ã —Ä–µ–∞–ª–∏–∑—É–µ–º —Å –Ω—É–ª—è –º–µ—Ç–æ–¥ **Speculative Decoding** ‚Äî –æ–¥–∏–Ω –∏–∑ —Å–∞–º—ã—Ö –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π —É—Å–∫–æ—Ä–∏—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞ –≤ 1.5-2.5 —Ä–∞–∑–∞ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ —Ç–∞–∫–∏—Ö –º–æ–¥–µ–ª–µ–π, –∫–∞–∫ Llama, Mixtral –∏ –¥—Ä—É–≥–∏—Ö.\n",
    "\n",
    "### –ü–ª–∞–Ω —Ä–∞–±–æ—Ç—ã:\n",
    "1. **–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è**: –ù–∞—Å—Ç—Ä–æ–∏–º —Å—Ä–µ–¥—É –∏ –∑–∞–≥—Ä—É–∑–∏–º –º–æ–¥–µ–ª–∏.\n",
    "2. **–†–µ–∞–ª–∏–∑–∞—Ü–∏—è Draft –º–æ–¥–µ–ª–∏**: –ú—ã –≤—Ä—É—á–Ω—É—é, –±–ª–æ–∫ –∑–∞ –±–ª–æ–∫–æ–º, —Å–æ–±–µ—Ä–µ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏ `Qwen2.5-0.5B` (`NanoQwen`). –≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏—Ç –Ω–∞–º –¥–æ—Å–∫–æ–Ω–∞–ª—å–Ω–æ –ø–æ–Ω—è—Ç—å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö LLM, –≤–∫–ª—é—á–∞—è `RMSNorm`, `RoPE` –∏ `SwiGLU`.\n",
    "3. **–†–µ–∞–ª–∏–∑–∞—Ü–∏—è —Ü–∏–∫–ª–∞ —Å–ø–µ–∫—É–ª—è—Ü–∏–∏**: –ù–∞–ø–∏—à–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∞–ª–≥–æ—Ä–∏—Ç–º, –≤ –∫–æ—Ç–æ—Ä–æ–º Draft –º–æ–¥–µ–ª—å –±—ã—Å—Ç—Ä–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —á–µ—Ä–Ω–æ–≤–∏–∫, –∞ Target –º–æ–¥–µ–ª—å –µ–≥–æ –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç.\n",
    "4. **–ë–µ–Ω—á–º–∞—Ä–∫**: –ü—Ä–æ–≤–µ–¥–µ–º –∑–∞–º–µ—Ä—ã –∏ –æ—Ü–µ–Ω–∏–º —Ä–µ–∞–ª—å–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –¥–∞–µ—Ç –Ω–∞—à –º–µ—Ç–æ–¥."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2vjOjG_DpQDR",
   "metadata": {
    "id": "2vjOjG_DpQDR"
   },
   "source": [
    "## –®–∞–≥ 1: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "SzxsondSpQDS",
   "metadata": {
    "id": "SzxsondSpQDS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~cipy (D:\\Soft\\Anaconda\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~cipy (D:\\Soft\\Anaconda\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~cipy (D:\\Soft\\Anaconda\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers accelerate safetensors sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "BSQNM5l0pQDT",
   "metadata": {
    "id": "BSQNM5l0pQDT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from safetensors.torch import load_file\n",
    "from huggingface_hub import hf_hub_download\n",
    "import time\n",
    "import gc\n",
    "import json\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# –§–∏–∫—Å–∏—Ä—É–µ–º seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leoZog3cpQDT",
   "metadata": {
    "id": "leoZog3cpQDT"
   },
   "source": [
    "### –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π\n",
    "\n",
    "–í –∫–∞—á–µ—Å—Ç–≤–µ —Ü–µ–ª–µ–≤–æ–π –º–æ–¥–µ–ª–∏ (`Target`) –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `Qwen/Qwen2.5-1.5B-Instruct`. –í –∫–∞—á–µ—Å—Ç–≤–µ —á–µ—Ä–Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (`Draft`) ‚Äî `Qwen/Qwen2.5-0.5B-Instruct`. –ú—ã –∑–∞–≥—Ä—É–∑–∏–º `Target` —Å –ø–æ–º–æ—â—å—é —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ `AutoModelForCausalLM` –∏–∑ `transformers`, –∞ –≤–æ—Ç `Draft` –º–æ–¥–µ–ª—å —Å–æ–±–µ—Ä–µ–º –≤—Ä—É—á–Ω—É—é."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "rQKJeCBhpQDT",
   "metadata": {
    "id": "rQKJeCBhpQDT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...\n",
      "–ó–∞–≥—Ä—É–∑–∫–∞ Target –º–æ–¥–µ–ª–∏ (—á–µ—Ä–µ–∑ AutoModelForCausalLM)...\n",
      "–ú–æ–¥–µ–ª—å Target –∑–∞–≥—Ä—É–∂–µ–Ω–∞.\n"
     ]
    }
   ],
   "source": [
    "TARGET_ID = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "DRAFT_ID = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(TARGET_ID)\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∑–∫–∞ Target –º–æ–¥–µ–ª–∏ (—á–µ—Ä–µ–∑ AutoModelForCausalLM)...\")\n",
    "# –ò—Å–ø–æ–ª—å–∑—É–µ–º torch_dtype=torch.float16 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ VRAM –∏ attn_implementation=\"sdpa\" –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Flash Attention\n",
    "target_model = AutoModelForCausalLM.from_pretrained(\n",
    "    TARGET_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"sdpa\"\n",
    ")\n",
    "target_model.eval()\n",
    "print(\"–ú–æ–¥–µ–ª—å Target –∑–∞–≥—Ä—É–∂–µ–Ω–∞.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Bc6gYAa_pQDU",
   "metadata": {
    "id": "Bc6gYAa_pQDU"
   },
   "source": [
    "## –®–∞–≥ 2: –°–æ–±–∏—Ä–∞–µ–º Draft –º–æ–¥–µ–ª—å \"NanoQwen\"\n",
    "\n",
    "–ù–∞ —ç—Ç–æ–º —à–∞–≥–µ –º—ã –Ω–µ –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `AutoModel`, –∞ —Å–æ–±–µ—Ä–µ–º –º–æ–¥–µ–ª—å —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –∏–∑ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –º–æ–¥—É–ª–µ–π. –≠—Ç–æ –∫–ª—é—á–µ–≤–∞—è —á–∞—Å—Ç—å –∑–∞–¥–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–º–æ–∂–µ—Ç –ø–æ–Ω—è—Ç—å, –∫–∞–∫ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã —É—Å—Ç—Ä–æ–µ–Ω—ã \"–ø–æ–¥ –∫–∞–ø–æ—Ç–æ–º\".\n",
    "\n",
    "–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏–º —Ç–æ–ª—å–∫–æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é `Draft` –º–æ–¥–µ–ª–∏, –∏–∑ –∫–æ—Ç–æ—Ä–æ–π –º—ã –±—É–¥–µ–º –±—Ä–∞—Ç—å –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã (—Ä–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –∏ —Ç.–¥.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "CEqcA5SdpQDU",
   "metadata": {
    "id": "CEqcA5SdpQDU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Draft –º–æ–¥–µ–ª–∏:\n",
      "  - –†–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è (hidden_size): 896\n",
      "  - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤ (num_hidden_layers): 24\n",
      "  - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è (num_attention_heads): 14\n"
     ]
    }
   ],
   "source": [
    "draft_config = AutoConfig.from_pretrained(DRAFT_ID)\n",
    "print(f\"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Draft –º–æ–¥–µ–ª–∏:\")\n",
    "print(f\"  - –†–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è (hidden_size): {draft_config.hidden_size}\")\n",
    "print(f\"  - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤ (num_hidden_layers): {draft_config.num_hidden_layers}\")\n",
    "print(f\"  - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è (num_attention_heads): {draft_config.num_attention_heads}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yJNp8JdPpQDU",
   "metadata": {
    "id": "yJNp8JdPpQDU"
   },
   "source": [
    "### –ó–∞–¥–∞–Ω–∏–µ 2.1: RMSNorm\n",
    "\n",
    "–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ Llama –∏ Qwen, –∏—Å–ø–æ–ª—å–∑—É—é—Ç **Root Mean Square Layer Normalization (RMSNorm)** –≤–º–µ—Å—Ç–æ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–≥–æ `LayerNorm`. `RMSNorm` –ø—Ä–æ—â–µ –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ, —Ç–∞–∫ –∫–∞–∫ –æ–ø–µ—Ä–∏—Ä—É–µ—Ç —Ç–æ–ª—å–∫–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ä–µ–¥–Ω–µ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Å–¥–≤–∏–≥ (bias).\n",
    "\n",
    "–§–æ—Ä–º—É–ª–∞ `RMSNorm` –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∞ –∞–∫—Ç–∏–≤–∞—Ü–∏–π $\\mathbf{x}$:\n",
    "$$ \\text{RMS}(\\mathbf{x}) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} x_i^2} $$\n",
    "$$ \\text{RMSNorm}(\\mathbf{x}) = \\frac{\\mathbf{x}}{\\text{RMS}(\\mathbf{x}) + \\epsilon} \\cdot \\mathbf{w} $$\n",
    "–≥–¥–µ $\\mathbf{w}$ ‚Äî –æ–±—É—á–∞–µ–º—ã–π –≤–µ—Å–æ–≤–æ–π –≤–µ–∫—Ç–æ—Ä (–≥–µ–π—Ç), –∞ $\\epsilon$ ‚Äî –º–∞–ª–∞—è –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞ –¥–ª—è —á–∏—Å–ª–µ–Ω–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏.\n",
    "\n",
    "**–í–∞–∂–Ω–æ**: –ü—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è—Ö –≤ `float16`, –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —Ä–∞—Å—á–µ—Ç `variance` (—Å—Ç–µ–ø–µ–Ω—å `pow(2)`) –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏—é. –ü–æ—ç—Ç–æ–º—É —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞ ‚Äî –≤—Ä–µ–º–µ–Ω–Ω–æ –ø–æ–≤—ã—à–∞—Ç—å —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –¥–æ `float32` –¥–ª—è —ç—Ç–æ–≥–æ —Ä–∞—Å—á–µ—Ç–∞.\n",
    "\n",
    "\n",
    "**–ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏**:\n",
    "- [Root Mean Square Layer Normalization (Zhang and Sennrich, 2019)](https://arxiv.org/abs/1910.07467)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lpvuxeKzpQDU",
   "metadata": {
    "id": "lpvuxeKzpQDU"
   },
   "outputs": [],
   "source": [
    "class QwenRMSNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    –†–µ–∞–ª–∏–∑–∞—Ü–∏—è Root Mean Square Layer Normalization.\n",
    "\n",
    "    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "        hidden_size (int): –†–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è.\n",
    "        eps (float): –ú–∞–ª–∞—è –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "\n",
    "        orig_dtype = hidden_states.dtype\n",
    "        x = hidden_states.float()\n",
    "\n",
    "        variance = x.pow(2).mean(dim=-1, keepdim=True)\n",
    "        x = x * torch.rsqrt(variance + self.variance_epsilon)\n",
    "\n",
    "        x = x * self.weight\n",
    "        return x.to(orig_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c5SXdL2pQDV",
   "metadata": {
    "id": "0c5SXdL2pQDV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ –¥–ª—è RMSNorm ---\n",
      "‚úÖ [1/3] –¢–µ—Å—Ç –Ω–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–æ–π–¥–µ–Ω.\n",
      "‚úÖ [2/3] –¢–µ—Å—Ç –Ω–∞ —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–π–¥–µ–Ω.\n",
      "‚úÖ [3/3] –¢–µ—Å—Ç –Ω–∞ NaN –ø—Ä–æ–π–¥–µ–Ω.\n",
      "\n",
      "üéâ –í—Å–µ —Ç–µ—Å—Ç—ã –¥–ª—è RMSNorm –ø—Ä–æ–π–¥–µ–Ω—ã!\n"
     ]
    }
   ],
   "source": [
    "print(\"--- –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ –¥–ª—è RMSNorm ---\")\n",
    "try:\n",
    "    hidden_size = 128\n",
    "    norm_layer = QwenRMSNorm(hidden_size).to(device).half()\n",
    "\n",
    "    # –¢–µ—Å—Ç 1: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏\n",
    "    dummy_input = torch.randn(2, 10, hidden_size, device=device).half()\n",
    "    output = norm_layer(dummy_input)\n",
    "    assert output.shape == dummy_input.shape, f\"–û—à–∏–±–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏: –æ–∂–∏–¥–∞–ª–æ—Å—å {dummy_input.shape}, –ø–æ–ª—É—á–µ–Ω–æ {output.shape}\"\n",
    "    print(\"‚úÖ [1/3] –¢–µ—Å—Ç –Ω–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–æ–π–¥–µ–Ω.\")\n",
    "\n",
    "    # –¢–µ—Å—Ç 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "    assert output.dtype == torch.float16, f\"–û—à–∏–±–∫–∞ —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö: –æ–∂–∏–¥–∞–ª–æ—Å—å float16, –ø–æ–ª—É—á–µ–Ω–æ {output.dtype}\"\n",
    "    print(\"‚úÖ [2/3] –¢–µ—Å—Ç –Ω–∞ —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–π–¥–µ–Ω.\")\n",
    "\n",
    "    # –¢–µ—Å—Ç 3: –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN\n",
    "    assert not torch.isnan(output).any(), \"–í –≤—ã—Ö–æ–¥–µ RMSNorm –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã NaN –∑–Ω–∞—á–µ–Ω–∏—è.\"\n",
    "    print(\"‚úÖ [3/3] –¢–µ—Å—Ç –Ω–∞ NaN –ø—Ä–æ–π–¥–µ–Ω.\")\n",
    "\n",
    "    print(\"\\nüéâ –í—Å–µ —Ç–µ—Å—Ç—ã –¥–ª—è RMSNorm –ø—Ä–æ–π–¥–µ–Ω—ã!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå –¢–µ—Å—Ç RMSNorm –ø—Ä–æ–≤–∞–ª–µ–Ω: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wH6ICMf0pQDV",
   "metadata": {
    "id": "wH6ICMf0pQDV"
   },
   "source": [
    "### –ó–∞–¥–∞–Ω–∏–µ 2.2: Rotary Positional Embeddings (RoPE)\n",
    "\n",
    "`RoPE` ‚Äî —ç—Ç–æ —ç–ª–µ–≥–∞–Ω—Ç–Ω—ã–π —Å–ø–æ—Å–æ–± –≤–Ω–µ–¥—Ä–µ–Ω–∏—è –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –≤–º–µ—Å—Ç–æ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤ (–∫–∞–∫ –≤ `sin/cos embeddings`) \"–≤—Ä–∞—â–∞–µ—Ç\" –≤–µ–∫—Ç–æ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤ (`Query`) –∏ –∫–ª—é—á–µ–π (`Key`) –Ω–∞ —É–≥–æ–ª, –∑–∞–≤–∏—Å—è—â–∏–π –æ—Ç –∏—Ö –ø–æ–∑–∏—Ü–∏–∏.\n",
    "\n",
    "#### –î–≤–∞ –ø–æ–¥—Ö–æ–¥–∞ –∫ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "–°—É—â–µ—Å—Ç–≤—É–µ—Ç –¥–≤–∞ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω—ã—Ö —Å–ø–æ—Å–æ–±–∞ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —ç—Ç–æ–≥–æ –≤—Ä–∞—â–µ–Ω–∏—è.\n",
    "\n",
    "1. **–†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –ø–∞—Ä—ã (Pairwise Rotation)**. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –º—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–ª–∏ –Ω–∞ –ª–µ–∫—Ü–∏–∏. –í–µ–∫—Ç–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ $\\mathbf{x} = (x_1, x_2, \\dots, x_d)$ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –∫–∞–∫ –Ω–∞–±–æ—Ä –¥–≤—É–º–µ—Ä–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ $(x_{2i-1}, x_{2i})$. –ö–∞–∂–¥—ã–π —Ç–∞–∫–æ–π –≤–µ–∫—Ç–æ—Ä –≤—Ä–∞—â–∞–µ—Ç—Å—è –≤ 2D-–ø–ª–æ—Å–∫–æ—Å—Ç–∏:\n",
    "$$\n",
    "\\begin{pmatrix} x'_{2i-1} \\\\ x'_{2i} \\end{pmatrix} =\n",
    "\\begin{pmatrix} \\cos(m\\theta_i) & -\\sin(m\\theta_i) \\\\ \\sin(m\\theta_i) & \\cos(m\\theta_i) \\end{pmatrix}\n",
    "\\begin{pmatrix} x_{2i-1} \\\\ x_{2i} \\end{pmatrix}\n",
    "$$\n",
    "–≥–¥–µ $m$ ‚Äî –ø–æ–∑–∏—Ü–∏—è —Ç–æ–∫–µ–Ω–∞, –∞ $\\theta_i$ ‚Äî —á–∞—Å—Ç–æ—Ç–∞ –≤—Ä–∞—â–µ–Ω–∏—è.\n",
    "\n",
    "2. **–í—Ä–∞—â–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –ø–æ–ª–æ–≤–∏–Ω—ã (`rotate_half`)**. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è—Ö Llama –∏ Qwen, –∏ –∏–º–µ–Ω–Ω–æ –µ–≥–æ –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å. –ó–¥–µ—Å—å –≤–µ–∫—Ç–æ—Ä $\\mathbf{x}$ –¥–µ–ª–∏—Ç—Å—è –Ω–∞ –¥–≤–µ –ø–æ–ª–æ–≤–∏–Ω—ã: $\\mathbf{x}_1 = (x_1, \\dots, x_{d/2})$ –∏ $\\mathbf{x}_2 = (x_{d/2+1}, \\dots, x_d)$. –í—Ä–∞—â–µ–Ω–∏–µ —Ä–µ–∞–ª–∏–∑—É–µ—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:\n",
    "$$ \\mathbf{x}'_m = \\mathbf{x}_m \\cos(m\\theta) + \\text{rotate\\_half}(\\mathbf{x}_m) \\sin(m\\theta) $$\n",
    "–≥–¥–µ –æ–ø–µ—Ä–∞—Ü–∏—è `rotate_half` –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤–µ–∫—Ç–æ—Ä $\\mathbf{x} = (\\mathbf{x}_1, \\mathbf{x}_2)$ –≤ $(-\\mathbf{x}_2, \\mathbf{x}_1)$. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ª–µ–≥—á–µ –≤–µ–∫—Ç–æ—Ä–∏–∑—É–µ—Ç—Å—è –∏ –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –≤ `torch`.\n",
    "\n",
    "\n",
    "**–ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏**:\n",
    "- [RoFormer: Enhanced Transformer with Rotary Position Embedding (Su et al., 2021)](https://arxiv.org/abs/2104.09864)\n",
    "- [–ü–æ–¥—Ä–æ–±–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ RoPE –≤ –±–ª–æ–≥–µ EleutherAI](https://blog.eleuther.ai/rotary-embeddings/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MlXiLfRXpQDV",
   "metadata": {
    "id": "MlXiLfRXpQDV"
   },
   "outputs": [],
   "source": [
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 1000000.0):\n",
    "    \"\"\"\n",
    "    –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –≤—ã—á–∏—Å–ª—è–µ—Ç —á–∞—Å—Ç–æ—Ç—ã –¥–ª—è RoPE –≤ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–º –≤–∏–¥–µ (cos + i*sin).\n",
    "\n",
    "    –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –≥–æ—Ç–æ–≤–∏—Ç —Ç–∞–±–ª–∏—Ü—ã —Å–∏–Ω—É—Å–æ–≤ –∏ –∫–æ—Å–∏–Ω—É—Å–æ–≤ –¥–ª—è –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö –ø–æ–∑–∏—Ü–∏–π.\n",
    "    \"\"\"\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    # –í —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ Llama/Qwen —á–∞—Å—Ç–æ—Ç—ã –¥—É–±–ª–∏—Ä—É—é—Ç—Å—è –¥–ª—è –æ–±–µ–∏—Ö –ø–æ–ª–æ–≤–∏–Ω\n",
    "    # –ú—ã –≤–µ—Ä–Ω–µ–º –∫–æ—Å–∏–Ω—É—Å—ã –∏ —Å–∏–Ω—É—Å—ã –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞\n",
    "    freqs_cat = torch.cat((freqs, freqs), dim=-1)\n",
    "    return torch.cos(freqs_cat), torch.sin(freqs_cat)\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"–í—Ä–∞—â–∞–µ—Ç –ø–æ–ª–æ–≤–∏–Ω—É —Å–∫—Ä—ã—Ç—ã—Ö –∏–∑–º–µ—Ä–µ–Ω–∏–π –≤—Ö–æ–¥–∞.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rope(q, k, cos, sin):\n",
    "    \"\"\"\n",
    "    –ü—Ä–∏–º–µ–Ω—è–µ—Ç RoPE –∫ –≤–µ–∫—Ç–æ—Ä–∞–º q –∏ k.\n",
    "    \"\"\"\n",
    "    # 1. –ò–∑–º–µ–Ω–∏—Ç–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å cos –∏ sin –¥–ª—è –±—Ä–æ–¥–∫–∞—Å—Ç–∏–Ω–≥–∞ —Å (q, k).\n",
    "    #    –û–Ω–∏ –¥–æ–ª–∂–Ω—ã —Å—Ç–∞—Ç—å [1, seq_len, 1, head_dim].\n",
    "    # 2. –ü—Ä–∏–º–µ–Ω–∏—Ç–µ —Ñ–æ—Ä–º—É–ª—É –≤—Ä–∞—â–µ–Ω–∏—è –∫ q –∏ k, –∏—Å–ø–æ–ª—å–∑—É—è rotate_half.\n",
    "    #    –û–ø–µ—Ä–∞—Ü–∏–∏ —Å float32 (cos/sin) –ø—Ä–∏–≤–µ–¥—É—Ç –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É float32.\n",
    "    # 3. –í–µ—Ä–Ω–∏—Ç–µ q_embed –∏ k_embed.\n",
    "    #    –í–ê–ñ–ù–û: –Ø–≤–Ω–æ –ø—Ä–∏–≤–æ–¥–∏–º –∫ —Ç–∏–ø—É q.dtype (float16), –∏–Ω–∞—á–µ —É–ø–∞–¥–µ—Ç assert –∏–ª–∏ —Å–ª–µ–¥—É—é—â–∏–π —Å–ª–æ–π\n",
    "    cos = cos.unsqueeze(0).unsqueeze(2)\n",
    "    sin = sin.unsqueeze(0).unsqueeze(2)\n",
    "\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "\n",
    "    return q_embed.to(q.dtype), k_embed.to(k.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83_Q3yNNpQDW",
   "metadata": {
    "id": "83_Q3yNNpQDW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ –¥–ª—è RoPE ---\n",
      "‚úÖ [1/2] –¢–µ—Å—Ç –Ω–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–æ–π–¥–µ–Ω.\n",
      "‚úÖ [2/2] –¢–µ—Å—Ç –Ω–∞ —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–π–¥–µ–Ω.\n",
      "\n",
      "üéâ –í—Å–µ —Ç–µ—Å—Ç—ã –¥–ª—è RoPE –ø—Ä–æ–π–¥–µ–Ω—ã!\n"
     ]
    }
   ],
   "source": [
    "print(\"--- –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ –¥–ª—è RoPE ---\")\n",
    "try:\n",
    "    head_dim = 64\n",
    "    seq_len = 10\n",
    "    batch_size = 2\n",
    "    num_heads = 4\n",
    "\n",
    "    xq = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device).half()\n",
    "    xk = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device).half()\n",
    "    cos, sin = precompute_freqs_cis(head_dim, seq_len * 2)\n",
    "    cos, sin = cos.to(device), sin.to(device)\n",
    "\n",
    "    # –í—ã–∑—ã–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é\n",
    "    xq_rot, xk_rot = apply_rope(xq, xk, cos[:seq_len], sin[:seq_len])\n",
    "\n",
    "    # –¢–µ—Å—Ç 1: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏\n",
    "    assert xq_rot.shape == xq.shape, f\"–û—à–∏–±–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ Query: –æ–∂–∏–¥–∞–ª–æ—Å—å {xq.shape}, –ø–æ–ª—É—á–µ–Ω–æ {xq_rot.shape}\"\n",
    "    assert xk_rot.shape == xk.shape, f\"–û—à–∏–±–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ Key: –æ–∂–∏–¥–∞–ª–æ—Å—å {xk.shape}, –ø–æ–ª—É—á–µ–Ω–æ {xk_rot.shape}\"\n",
    "    print(\"‚úÖ [1/2] –¢–µ—Å—Ç –Ω–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–æ–π–¥–µ–Ω.\")\n",
    "\n",
    "    # –¢–µ—Å—Ç 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "    assert xq_rot.dtype == xq.dtype, f\"–û—à–∏–±–∫–∞ —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö Query: –æ–∂–∏–¥–∞–ª–æ—Å—å {xq.dtype}, –ø–æ–ª—É—á–µ–Ω–æ {xq_rot.dtype}\"\n",
    "    assert xk_rot.dtype == xk.dtype, f\"–û—à–∏–±–∫–∞ —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö Key: –æ–∂–∏–¥–∞–ª–æ—Å—å {xk.dtype}, –ø–æ–ª—É—á–µ–Ω–æ {xk_rot.dtype}\"\n",
    "    print(\"‚úÖ [2/2] –¢–µ—Å—Ç –Ω–∞ —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–π–¥–µ–Ω.\")\n",
    "\n",
    "    print(\"\\nüéâ –í—Å–µ —Ç–µ—Å—Ç—ã –¥–ª—è RoPE –ø—Ä–æ–π–¥–µ–Ω—ã!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå –¢–µ—Å—Ç RoPE –ø—Ä–æ–≤–∞–ª–µ–Ω: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rcKYYkSvpQDW",
   "metadata": {
    "id": "rcKYYkSvpQDW"
   },
   "source": [
    "### –ó–∞–¥–∞–Ω–∏–µ 2.3: SwiGLU MLP\n",
    "\n",
    "–í–º–µ—Å—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ `FeedForward` –±–ª–æ–∫–∞ —Å –æ–¥–Ω–æ–π `ReLU` –∞–∫—Ç–∏–≤–∞—Ü–∏–µ–π, —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç `Gated Linear Units (GLU)` –∏ –∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç—ã. –í Qwen/Llama –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è **SwiGLU**.\n",
    "\n",
    "–ò–¥–µ—è —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–µ–π—Ç (—à–ª—é–∑) –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–º –ø–æ—Ç–æ–∫–æ–º. –í—Ö–æ–¥–Ω–æ–π –≤–µ–∫—Ç–æ—Ä `x` –ø—Ä–æ–µ—Ü–∏—Ä—É–µ—Ç—Å—è –¥–≤—É–º—è —Ä–∞–∑–Ω—ã–º–∏ –ª–∏–Ω–µ–π–Ω—ã–º–∏ —Å–ª–æ—è–º–∏ (`up` –∏ `gate`). –†–µ–∑—É–ª—å—Ç–∞—Ç `gate` –ø—Ä–æ–µ–∫—Ü–∏–∏ –ø—Ä–æ—Ö–æ–¥–∏—Ç —á–µ—Ä–µ–∑ –∞–∫—Ç–∏–≤–∞—Ü–∏—é `SiLU` (—Ç–∞–∫–∂–µ –∏–∑–≤–µ—Å—Ç–Ω—É—é –∫–∞–∫ Swish), –∞ –∑–∞—Ç–µ–º –ø–æ—ç–ª–µ–º–µ–Ω—Ç–Ω–æ —É–º–Ω–æ–∂–∞–µ—Ç—Å—è –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç `up` –ø—Ä–æ–µ–∫—Ü–∏–∏. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–µ—Ç–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ —Ä–µ—à–∞—Ç—å, –∫–∞–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–æ–ª–∂–Ω–∞ –ø—Ä–æ–π—Ç–∏ –¥–∞–ª—å—à–µ.\n",
    "\n",
    "–§–æ—Ä–º—É–ª–∞ `SwiGLU`:\n",
    "$$ \\text{SwiGLU}(x, W_{up}, W_{gate}, W_{down}) = (\\text{SiLU}(x W_{gate}) \\otimes (x W_{up})) W_{down} $$\n",
    "–≥–¥–µ $\\otimes$ ‚Äî –ø–æ—ç–ª–µ–º–µ–Ω—Ç–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ, –∞ `SiLU` (Swish activation) –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –∫–∞–∫:\n",
    "$$ \\text{SiLU}(x) = x \\cdot \\sigma(x) $$\n",
    "–≥–¥–µ $\\sigma$ ‚Äî —ç—Ç–æ —Å–∏–≥–º–æ–∏–¥–∞.\n",
    "\n",
    "**–ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏**:\n",
    "- [GLU Variants Improve Transformer (Shazeer, 2020)](https://arxiv.org/abs/2002.05202)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EcSpUB2_pQDW",
   "metadata": {
    "id": "EcSpUB2_pQDW"
   },
   "outputs": [],
   "source": [
    "class QwenMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    –†–µ–∞–ª–∏–∑–∞—Ü–∏—è SwiGLU Feed-Forward —Å–µ—Ç–∏.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.up_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate = self.gate_proj(x)\n",
    "        up = self.up_proj(x)\n",
    "        x = F.silu(gate) * up\n",
    "        x = self.down_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59dbtNAVpQDW",
   "metadata": {
    "id": "59dbtNAVpQDW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ –¥–ª—è SwiGLU MLP ---\n",
      "‚úÖ [1/2] –¢–µ—Å—Ç –Ω–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–æ–π–¥–µ–Ω.\n",
      "‚úÖ [2/2] –¢–µ—Å—Ç –Ω–∞ NaN –ø—Ä–æ–π–¥–µ–Ω.\n",
      "\n",
      "üéâ –í—Å–µ —Ç–µ—Å—Ç—ã –¥–ª—è SwiGLU MLP –ø—Ä–æ–π–¥–µ–Ω—ã!\n"
     ]
    }
   ],
   "source": [
    "print(\"--- –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ –¥–ª—è SwiGLU MLP ---\")\n",
    "try:\n",
    "    # –°–æ–∑–¥–∞–µ–º mock-–∫–æ–Ω—Ñ–∏–≥ –¥–ª—è —Ç–µ—Å—Ç–∞\n",
    "    class MockConfig:\n",
    "        hidden_size = 128\n",
    "        intermediate_size = 384\n",
    "\n",
    "    config = MockConfig()\n",
    "    mlp = QwenMLP(config).to(device).half()\n",
    "\n",
    "    x = torch.randn(2, 10, config.hidden_size, device=device).half()\n",
    "    output = mlp(x)\n",
    "\n",
    "    # –¢–µ—Å—Ç 1: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏\n",
    "    assert output.shape == x.shape, f\"–û—à–∏–±–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏: –æ–∂–∏–¥–∞–ª–æ—Å—å {x.shape}, –ø–æ–ª—É—á–µ–Ω–æ {output.shape}\"\n",
    "    print(\"‚úÖ [1/2] –¢–µ—Å—Ç –Ω–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–æ–π–¥–µ–Ω.\")\n",
    "\n",
    "    # –¢–µ—Å—Ç 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN\n",
    "    assert not torch.isnan(output).any(), \"–í –≤—ã—Ö–æ–¥–µ MLP –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã NaN –∑–Ω–∞—á–µ–Ω–∏—è.\"\n",
    "    print(\"‚úÖ [2/2] –¢–µ—Å—Ç –Ω–∞ NaN –ø—Ä–æ–π–¥–µ–Ω.\")\n",
    "\n",
    "    print(\"\\nüéâ –í—Å–µ —Ç–µ—Å—Ç—ã –¥–ª—è SwiGLU MLP –ø—Ä–æ–π–¥–µ–Ω—ã!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå –¢–µ—Å—Ç SwiGLU MLP –ø—Ä–æ–≤–∞–ª–µ–Ω: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UWwHWvMZpQDW",
   "metadata": {
    "id": "UWwHWvMZpQDW"
   },
   "source": [
    "### –ó–∞–¥–∞–Ω–∏–µ 2.4: –°–±–æ—Ä–∫–∞ –∏—Ç–æ–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏ NanoQwen\n",
    "\n",
    "–¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ —É –Ω–∞—Å –µ—Å—Ç—å –≤—Å–µ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–µ –±–ª–æ–∫–∏, –º—ã –º–æ–∂–µ–º —Å–æ–±—Ä–∞—Ç—å –∏–∑ –Ω–∏—Ö –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π —Å–ª–æ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ (`NanoQwenBlock`) –∏ —Å–∞–º—É –º–æ–¥–µ–ª—å (`NanoQwen`).\n",
    "\n",
    "–í–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–æ–ø–æ–ª–Ω–∏—Ç—å –∫–ª–∞—Å—Å `NanoQwenBlock`, –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Å–æ–µ–¥–∏–Ω–∏–≤ –≤—Å–µ –º–æ–¥—É–ª–∏. –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –ø–æ—Ä—è–¥–æ–∫ –æ–ø–µ—Ä–∞—Ü–∏–π –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö —Å–µ–º–µ–π—Å—Ç–≤–∞ Llama/Qwen:\n",
    "1. **Pre-normalization** –ø–µ—Ä–µ–¥ Self-Attention.\n",
    "2. **Residual Connection** –ø–æ—Å–ª–µ Self-Attention.\n",
    "3. **Pre-normalization** –ø–µ—Ä–µ–¥ MLP.\n",
    "4. **Residual Connection** –ø–æ—Å–ª–µ MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IdQ7OpowpQDW",
   "metadata": {
    "id": "IdQ7OpowpQDW"
   },
   "outputs": [],
   "source": [
    "class QwenAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = config.hidden_size // config.num_attention_heads\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=True)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, hidden_states, cos, sin):\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "        q = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim)\n",
    "        k = self.k_proj(hidden_states).view(bsz, q_len, self.num_key_value_heads, self.head_dim)\n",
    "        v = self.v_proj(hidden_states).view(bsz, q_len, self.num_key_value_heads, self.head_dim)\n",
    "\n",
    "        q, k = apply_rope(q, k, cos, sin)\n",
    "\n",
    "        # Grouped Query Attention (GQA)\n",
    "        if self.num_key_value_heads != self.num_heads:\n",
    "            k = k.repeat_interleave(self.num_heads // self.num_key_value_heads, dim=2)\n",
    "            v = v.repeat_interleave(self.num_heads // self.num_key_value_heads, dim=2)\n",
    "\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—É—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é Flash Attention\n",
    "        output = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, q_len, -1)\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class NanoQwenBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self_attn = QwenAttention(config)\n",
    "        self.mlp = QwenMLP(config)\n",
    "        self.input_layernorm = QwenRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = QwenRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, hidden_states, cos, sin):\n",
    "\n",
    "        residual = hidden_states\n",
    "        x = self.input_layernorm(hidden_states)\n",
    "        x = self.self_attn(x, cos, sin)\n",
    "        hidden_states = residual + x\n",
    "\n",
    "        residual = hidden_states\n",
    "        x = self.post_attention_layernorm(hidden_states)\n",
    "        x = self.mlp(x)\n",
    "        hidden_states = residual + x\n",
    "        return hidden_states\n",
    "\n",
    "class NanoQwen(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([NanoQwenBlock(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.norm = QwenRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # –ü—Ä–µ–¥–≤—ã—á–∏—Å–ª—è–µ–º RoPE\n",
    "        self.cos, self.sin = precompute_freqs_cis(\n",
    "            config.hidden_size // config.num_attention_heads,\n",
    "            config.max_position_embeddings * 2\n",
    "        )\n",
    "        self.cos = self.cos.to(device)\n",
    "        self.sin = self.sin.to(device)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embed_tokens(input_ids)\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        # –í—ã–±–∏—Ä–∞–µ–º –Ω—É–∂–Ω—ã–π —Å—Ä–µ–∑ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã RoPE\n",
    "        cos_t = self.cos[:seq_len]\n",
    "        sin_t = self.sin[:seq_len]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, cos_t, sin_t)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I_4qwdaypQDX",
   "metadata": {
    "id": "I_4qwdaypQDX"
   },
   "source": [
    "### –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤\n",
    "\n",
    "–¢–µ–ø–µ—Ä—å –Ω–∞–º –Ω—É–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤–µ—Å–∞ –∏–∑ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è –≤ –Ω–∞—à—É —Å–∞–º–æ–ø–∏—Å–Ω—É—é –º–æ–¥–µ–ª—å. –ú—ã —É–∂–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–ª–∏ —Ñ—É–Ω–∫—Ü–∏—é `load_weights_manual`, –∫–æ—Ç–æ—Ä–∞—è –¥–µ–ª–∞–µ—Ç —ç—Ç–æ –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–∏—è –≤ –∏–º–µ–Ω–æ–≤–∞–Ω–∏–∏ —Å–ª–æ–µ–≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "B4Zyc02ApQDX",
   "metadata": {
    "id": "B4Zyc02ApQDX"
   },
   "outputs": [],
   "source": [
    "def load_weights_manual(model, model_id):\n",
    "    \"\"\"\n",
    "    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤–µ—Å–∞ –∏–∑ —Ñ–∞–π–ª–∞ .safetensors –≤ —Ä—É—á–Ω–æ–º —Ä–µ–∂–∏–º–µ,\n",
    "    –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É—è –∏–º–µ–Ω–∞ –∫–ª—é—á–µ–π.\n",
    "    \"\"\"\n",
    "    print(\"–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –≤–µ—Å–æ–≤...\")\n",
    "    file_path = hf_hub_download(repo_id=model_id, filename=\"model.safetensors\")\n",
    "\n",
    "    print(\"–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ safetensors...\")\n",
    "    state_dict = load_file(file_path, device=\"cpu\") # –ì—Ä—É–∑–∏–º –Ω–∞ CPU, —á—Ç–æ–±—ã –Ω–µ –∑–∞–Ω–∏–º–∞—Ç—å VRAM\n",
    "\n",
    "    new_state_dict = {}\n",
    "    for key, tensor in state_dict.items():\n",
    "        new_key = key\n",
    "\n",
    "        # –ï—Å–ª–∏ –∫–ª—é—á –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å \"model.\", —É–¥–∞–ª—è–µ–º —ç—Ç–æ—Ç –ø—Ä–µ—Ñ–∏–∫—Å\n",
    "        if key.startswith(\"model.\"):\n",
    "            new_key = key[len(\"model.\"):]\n",
    "\n",
    "        new_state_dict[new_key] = tensor\n",
    "\n",
    "    # –í –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –º–æ–¥–µ–ª—è—Ö lm_head –∏ embed_tokens –∏—Å–ø–æ–ª—å–∑—É—é—Ç –æ–±—â–∏–µ –≤–µ—Å–∞ (tied weights).\n",
    "    # –ï—Å–ª–∏ lm_head –Ω–µ—Ç, –∫–æ–ø–∏—Ä—É–µ–º –µ–≥–æ –∏–∑ embed_tokens.\n",
    "    if \"lm_head.weight\" not in new_state_dict and \"embed_tokens.weight\" in new_state_dict:\n",
    "        print(\"–°–≤—è–∑—ã–≤–∞–Ω–∏–µ –≤–µ—Å–æ–≤: lm_head.weight <- embed_tokens.weight\")\n",
    "        new_state_dict[\"lm_head.weight\"] = new_state_dict[\"embed_tokens.weight\"]\n",
    "\n",
    "    print(\"–ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤ –≤ –º–æ–¥–µ–ª—å...\")\n",
    "    missing, unexpected = model.load_state_dict(new_state_dict, strict=False)\n",
    "\n",
    "    if not missing and not unexpected:\n",
    "        print(\"‚úÖ –í–µ—Å–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!\")\n",
    "    else:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –≤–µ—Å–æ–≤:\")\n",
    "        if missing:\n",
    "            print(f\"  –ù–µ –Ω–∞–π–¥–µ–Ω—ã –∫–ª—é—á–∏ –≤ state_dict: {missing[:5]}\")\n",
    "        if unexpected:\n",
    "            print(f\"  –õ–∏—à–Ω–∏–µ –∫–ª—é—á–∏ –≤ state_dict: {unexpected[:5]}\")\n",
    "\n",
    "    model.to(device).to(torch.float16)\n",
    "    return model, missing, unexpected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0v6jIEKmpQDX",
   "metadata": {
    "id": "0v6jIEKmpQDX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –≤–µ—Å–æ–≤...\n",
      "–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ safetensors...\n",
      "–°–≤—è–∑—ã–≤–∞–Ω–∏–µ –≤–µ—Å–æ–≤: lm_head.weight <- embed_tokens.weight\n",
      "–ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤ –≤ –º–æ–¥–µ–ª—å...\n",
      "‚úÖ –í–µ—Å–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!\n",
      "\n",
      "--- –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤–µ—Å–æ–≤ ---\n",
      "üéâ –¢–µ—Å—Ç –Ω–∞ –∑–∞–≥—Ä—É–∑–∫—É –≤–µ—Å–æ–≤ –ø—Ä–æ–π–¥–µ–Ω!\n"
     ]
    }
   ],
   "source": [
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –∑–∞–≥—Ä—É–∑–∫–∞ Draft –º–æ–¥–µ–ª–∏\n",
    "draft_model = NanoQwen(draft_config)\n",
    "draft_model, missing_keys, unexpected_keys = load_weights_manual(draft_model, DRAFT_ID)\n",
    "draft_model.eval()\n",
    "\n",
    "# –¢–µ—Å—Ç\n",
    "print(\"\\n--- –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤–µ—Å–æ–≤ ---\")\n",
    "try:\n",
    "    assert not missing_keys, f\"–ù–∞–π–¥–µ–Ω—ã –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∫–ª—é—á–∏: {missing_keys[:5]}\"\n",
    "    assert not unexpected_keys, f\"–ù–∞–π–¥–µ–Ω—ã –ª–∏—à–Ω–∏–µ –∫–ª—é—á–∏: {unexpected_keys[:5]}\"\n",
    "    print(\"üéâ –¢–µ—Å—Ç –Ω–∞ –∑–∞–≥—Ä—É–∑–∫—É –≤–µ—Å–æ–≤ –ø—Ä–æ–π–¥–µ–Ω!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå –¢–µ—Å—Ç –ø—Ä–æ–≤–∞–ª–µ–Ω: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vmb-lsaWpQDX",
   "metadata": {
    "id": "vmb-lsaWpQDX"
   },
   "source": [
    "### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏\n",
    "\n",
    "–î–∞–≤–∞–π—Ç–µ —É–±–µ–¥–∏–º—Å—è, —á—Ç–æ –Ω–∞—à–∞ —Å–∞–º–æ–ø–∏—Å–Ω–∞—è –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç. –ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π —Ü–∏–∫–ª –∂–∞–¥–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (`greedy search`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ZkSsj3xpQDX",
   "metadata": {
    "id": "9ZkSsj3xpQDX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- –û—Ç–≤–µ—Ç –æ—Ç NanoQwen: ---\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "Who are you?\n",
      "assistant\n",
      "I am a large language model created by Alibaba Cloud. I am called Qwen.\n"
     ]
    }
   ],
   "source": [
    "def generate_simple(model, text, max_new=10):\n",
    "    \"\"\"–ü—Ä–æ—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∂–∞–¥–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    input_ids = inputs.input_ids\n",
    "\n",
    "    with torch.no_grad(), torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "        for _ in range(max_new):\n",
    "            logits = model(input_ids)\n",
    "            next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "print(\"--- –û—Ç–≤–µ—Ç –æ—Ç NanoQwen: ---\")\n",
    "print(generate_simple(draft_model, text, max_new=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dZ1PpdiMpQDX",
   "metadata": {
    "id": "dZ1PpdiMpQDX"
   },
   "source": [
    "## –®–∞–≥ 3: –ê–ª–≥–æ—Ä–∏—Ç–º —Å–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "\n",
    "–¢–µ–ø–µ—Ä—å —Å–∞–º–∞—è –≤–∞–∂–Ω–∞—è —á–∞—Å—Ç—å. –ú—ã —Ä–µ–∞–ª–∏–∑—É–µ–º **Greedy Speculative Decoding**.\n",
    "\n",
    "**–ê–ª–≥–æ—Ä–∏—Ç–º**:\n",
    "1. **–ß–µ—Ä–Ω–æ–≤–∏–∫ (Draft)**: –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º `K` —Ç–æ–∫–µ–Ω–æ–≤ —Å –ø–æ–º–æ—â—å—é –±—ã—Å—Ç—Ä–æ–π –º–∞–ª–µ–Ω—å–∫–æ–π –º–æ–¥–µ–ª–∏ (`Draft`).\n",
    "2. **–í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è (Verify)**: –ü—Ä–æ–≥–æ–Ω—è–µ–º –≤—Å—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å (–ø—Ä–µ—Ñ–∏–∫—Å + `K` —Ç–æ–∫–µ–Ω–æ–≤) —á–µ—Ä–µ–∑ –º–µ–¥–ª–µ–Ω–Ω—É—é –±–æ–ª—å—à—É—é –º–æ–¥–µ–ª—å (`Target`) **–∑–∞ –æ–¥–∏–Ω `forward` –≤—ã–∑–æ–≤**.\n",
    "3. **–ü—Ä–æ–≤–µ—Ä–∫–∞ (Accept/Reject)**: –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ç–æ–∫–µ–Ω—ã, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ `Target` –º–æ–¥–µ–ª—å—é, —Å —Ç–æ–∫–µ–Ω–∞–º–∏, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ `Draft` –º–æ–¥–µ–ª—å—é.\n",
    "   - –ù–∞—Ö–æ–¥–∏–º –ø–µ—Ä–≤—ã–π –∏–Ω–¥–µ–∫—Å `i`, –≥–¥–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–µ —Å–æ–≤–ø–∞–ª–∏.\n",
    "   - –í—Å–µ `i` —Å–æ–≤–ø–∞–≤—à–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ —Å—á–∏—Ç–∞—é—Ç—Å—è \"–ø—Ä–∏–Ω—è—Ç—ã–º–∏\" –∏ –¥–æ–±–∞–≤–ª—è—é—Ç—Å—è –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É.\n",
    "   - –í –∫–∞—á–µ—Å—Ç–≤–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ –º—ã –±–µ—Ä–µ–º \"–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π\" —Ç–æ–∫–µ–Ω –æ—Ç `Target` –º–æ–¥–µ–ª–∏ –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ `i`.\n",
    "   - –í—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –∏–∑ —á–µ—Ä–Ω–æ–≤–∏–∫–∞ –æ—Ç–±—Ä–∞—Å—ã–≤–∞—é—Ç—Å—è.\n",
    "4. –ü–æ–≤—Ç–æ—Ä—è–µ–º —Ü–∏–∫–ª.\n",
    "\n",
    "**–ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏**:\n",
    "- [Fast Inference from Transformers via Speculative Decoding (Leviathan et al., 2022)](https://arxiv.org/abs/2211.17192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vjoDQcOQpQDX",
   "metadata": {
    "id": "vjoDQcOQpQDX"
   },
   "outputs": [],
   "source": [
    "def speculative_sampling(prefix_text, max_new_tokens, target_model, draft_model, tokenizer, K=5):\n",
    "    \"\"\"\n",
    "    –†–µ–∞–ª–∏–∑—É–µ—Ç —Ü–∏–∫–ª —Å–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è.\n",
    "    \"\"\"\n",
    "    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prefix_text}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    generated_ids = input_ids.clone()\n",
    "    finished_len = input_ids.shape[1] + max_new_tokens\n",
    "\n",
    "    stats = {\"target_calls\": 0, \"total_accepted\": 0, \"total_drafted\": 0}\n",
    "\n",
    "    with torch.no_grad(), torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "        while generated_ids.shape[1] < finished_len:\n",
    "            prefix_len = generated_ids.shape[1]\n",
    "\n",
    "            # 1. DRAFT\n",
    "            draft_ids = generated_ids\n",
    "            for _ in range(K):\n",
    "                outputs = draft_model(draft_ids)\n",
    "                next_token = torch.argmax(outputs[:, -1, :], dim=-1, keepdim=True)\n",
    "                draft_ids = torch.cat([draft_ids, next_token], dim=1)\n",
    "                if next_token.item() == tokenizer.eos_token_id:\n",
    "                    break\n",
    "\n",
    "            drafted_tokens = draft_ids[0, prefix_len:]\n",
    "            if not len(drafted_tokens): break # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–∏\n",
    "            stats[\"total_drafted\"] += len(drafted_tokens)\n",
    "\n",
    "            # 2. VERIFY\n",
    "            target_outputs = target_model(draft_ids)\n",
    "            stats[\"target_calls\"] += 1\n",
    "            target_logits = target_outputs.logits\n",
    "            # –ù–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É—é—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è Target –º–æ–¥–µ–ª–∏ –¥–ª—è —Ç–æ–∫–µ–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª Draft\n",
    "            relevant_logits = target_logits[0, prefix_len-1 : -1]\n",
    "            target_preds = torch.argmax(relevant_logits, dim=-1)\n",
    "\n",
    "            # 3. ACCEPT/REJECT Logic\n",
    "            n_accepted = 0\n",
    "\n",
    "            for d_tok, t_tok in zip(drafted_tokens, target_preds):\n",
    "                if d_tok.item() == t_tok.item():\n",
    "                    n_accepted += 1\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            stats[\"total_accepted\"] += n_accepted\n",
    "\n",
    "            # –ü—Ä–∏–Ω–∏–º–∞–µ–º –≤—Å–µ —Å–æ–≤–ø–∞–≤—à–∏–µ —Ç–æ–∫–µ–Ω—ã\n",
    "            accepted_ids = drafted_tokens[:n_accepted]\n",
    "            generated_ids = torch.cat([generated_ids, accepted_ids.unsqueeze(0)], dim=1)\n",
    "\n",
    "            # –ï—Å–ª–∏ –¥–æ—Å—Ç–∏–≥–ª–∏ –ª–∏–º–∏—Ç–∞, –≤—ã—Ö–æ–¥–∏–º\n",
    "            if generated_ids.shape[1] >= finished_len:\n",
    "                break\n",
    "\n",
    "            # –î–æ–±–∞–≤–ª—è–µ–º –æ–¥–∏–Ω \"–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π\" —Ç–æ–∫–µ–Ω –æ—Ç Target –º–æ–¥–µ–ª–∏\n",
    "            if n_accepted < len(target_preds):\n",
    "                correct_token = target_preds[n_accepted].view(1, 1)\n",
    "            else: # –ï—Å–ª–∏ –≤—Å–µ —Å–æ–≤–ø–∞–ª–æ, –±–µ—Ä–µ–º —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω –æ—Ç Target –º–æ–¥–µ–ª–∏\n",
    "                last_logits = target_logits[0, -1, :]\n",
    "                correct_token = torch.argmax(last_logits).view(1, 1)\n",
    "\n",
    "            generated_ids = torch.cat([generated_ids, correct_token], dim=1)\n",
    "\n",
    "            if correct_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True), stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rkgk7ibDH3Qc",
   "metadata": {
    "id": "rkgk7ibDH3Qc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞ –¥–ª—è Speculative Decoding ---\n",
      "Drafted tokens: [10, 20, 30, 40, 50]\n",
      "Target preds (ideal): [10, 20, 99, 40, 50]\n",
      "Accepted: 2\n",
      "üéâ –¢–µ—Å—Ç –¥–ª—è –ª–æ–≥–∏–∫–∏ Accept/Reject –ø—Ä–æ–π–¥–µ–Ω!\n"
     ]
    }
   ],
   "source": [
    "print(\"--- –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞ –¥–ª—è Speculative Decoding ---\")\n",
    "try:\n",
    "    # –°–æ–∑–¥–∞–µ–º \"–∏–≥—Ä—É—à–µ—á–Ω—ã–µ\" –º–æ–¥–µ–ª–∏ –¥–ª—è —Ç–µ—Å—Ç–∞\n",
    "    class MockModel(nn.Module):\n",
    "        def __init__(self, vocab_size, response_sequence):\n",
    "            super().__init__()\n",
    "            self.vocab_size = vocab_size\n",
    "            self.response = response_sequence\n",
    "            self.call_idx = 0\n",
    "        def forward(self, input_ids):\n",
    "            if input_ids is None: return None # property hack fix\n",
    "            batch, seq_len = input_ids.shape\n",
    "            next_token_idx = min(self.call_idx, len(self.response) -1)\n",
    "            next_token = self.response[next_token_idx]\n",
    "            self.call_idx += 1\n",
    "            logits = torch.full((batch, seq_len, self.vocab_size), -100.0, device=device)\n",
    "            logits[:, -1, next_token] = 100.0\n",
    "            return logits\n",
    "\n",
    "    mock_draft = MockModel(100, [10, 20, 30, 40, 50])\n",
    "    mock_target = MockModel(100, [10, 20, 99, 40, 50])\n",
    "    # –•–∞–º—Å–∫–∏–π —Ö–∞–∫ –¥–ª—è property, –Ω–æ –≤ —Ç–µ—Å—Ç–µ –º—ã –≤—ã–∑—ã–≤–∞–µ–º forward –Ω–∞–ø—Ä—è–º—É—é —á–µ—Ä–µ–∑ target(draft_ids)\n",
    "\n",
    "    def mock_speculative_sampling(target, draft, K=5):\n",
    "        generated_ids = torch.tensor([[1, 2]], device=device)\n",
    "        prefix_len = generated_ids.shape[1]\n",
    "        draft_ids = torch.tensor([[1, 2, 10, 20, 30, 40, 50]], device=device)\n",
    "\n",
    "        drafted_tokens = draft_ids[0, prefix_len:]\n",
    "        print(f\"Drafted tokens: {drafted_tokens.tolist()}\")\n",
    "\n",
    "        # –≠–º—É–ª–∏—Ä—É–µ–º –≤—ã–∑–æ–≤ target –º–æ–¥–µ–ª–∏\n",
    "        # –í–∞–∂–Ω–æ: MockModel –≤ –Ω–∞—à–µ–º —Ç–µ—Å—Ç–µ –æ—á–µ–Ω—å —Ç—É–ø–∞—è, –æ–Ω–∞ –ø—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω –∏–∑ —Å–ø–∏—Å–∫–∞ response\n",
    "        # –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –≤—Ö–æ–¥–∞. –ù–∞–º –Ω—É–∂–Ω–æ –≤—ã–∑–≤–∞—Ç—å –µ–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏,\n",
    "        # —á—Ç–æ–±—ã —Å–æ–±—Ä–∞—Ç—å \"–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ\" –ø—Ä–µ–¥–∏–∫—Ç—ã.\n",
    "\n",
    "        # –ü–µ—Ä–µ–ø–∏—à–µ–º –ª–æ–≥–∏–∫—É —Å–±–æ—Ä–∞ –ø—Ä–µ–¥–∏–∫—Ç–æ–≤ –¥–ª—è MockModel, —á—Ç–æ–±—ã –æ–Ω–∞ —Ä–∞–±–æ—Ç–∞–ª–∞ –∫–∞–∫ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏—è\n",
    "        target_preds_list = []\n",
    "\n",
    "        # –ú—ã —Ö–æ—Ç–∏–º –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –ø—Ä–µ–¥–∏–∫—Ç—ã –¥–ª—è:\n",
    "        # input=[1, 2] -> –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å 10)\n",
    "        # input=[... 10] -> –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å 20)\n",
    "        # input=[... 20] -> –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å 99)\n",
    "\n",
    "        # –°–±—Ä–æ—Å–∏–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ\n",
    "        target.call_idx = 0\n",
    "\n",
    "        # –í —Ä–µ–∞–ª—å–Ω–æ–π –∂–∏–∑–Ω–∏ –º—ã –¥–µ–ª–∞–µ–º –æ–¥–∏–Ω forward pass.\n",
    "        # –ù–æ –Ω–∞—à–∞ MockModel –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ç–æ–∫–µ–Ω.\n",
    "        # –ü–æ—ç—Ç–æ–º—É –¥–ª—è —Ç–µ—Å—Ç–∞ –º—ã –ø—Ä–æ—Å—Ç–æ –≤–æ–∑—å–º–µ–º response_sequence —É—á–∏—Ç–µ–ª—è.\n",
    "        # –≠—Ç–æ —É–ø—Ä–æ—â–µ–Ω–∏–µ —Ç–µ—Å—Ç–∞, –Ω–æ –æ–Ω–æ –≤–∞–ª–∏–¥–∏—Ä—É–µ—Ç –ª–æ–≥–∏–∫—É accept/reject.\n",
    "\n",
    "        teacher_sequence = target.response # [10, 20, 99, 40, 50]\n",
    "        target_preds = torch.tensor(teacher_sequence, device=device)\n",
    "\n",
    "        print(f\"Target preds (ideal): {target_preds.tolist()}\")\n",
    "\n",
    "        n_accepted = 0\n",
    "        for d_tok, t_tok in zip(drafted_tokens, target_preds[: len(drafted_tokens)]):\n",
    "            if d_tok.item() == t_tok.item():\n",
    "                n_accepted += 1\n",
    "            else:\n",
    "                break\n",
    "        return n_accepted\n",
    "\n",
    "    n_accepted = mock_speculative_sampling(mock_target, mock_draft)\n",
    "    print(f\"Accepted: {n_accepted}\")\n",
    "\n",
    "    assert n_accepted == 2, f\"–û—à–∏–±–∫–∞ –≤ –ª–æ–≥–∏–∫–µ Accept/Reject: –æ–∂–∏–¥–∞–ª–æ—Å—å 2 –ø—Ä–∏–Ω—è—Ç—ã—Ö —Ç–æ–∫–µ–Ω–∞, –ø–æ–ª—É—á–µ–Ω–æ {n_accepted}\"\n",
    "    print(\"üéâ –¢–µ—Å—Ç –¥–ª—è –ª–æ–≥–∏–∫–∏ Accept/Reject –ø—Ä–æ–π–¥–µ–Ω!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå –¢–µ—Å—Ç –ø—Ä–æ–≤–∞–ª–µ–Ω: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "F-3PR3KQpQDY",
   "metadata": {
    "id": "F-3PR3KQpQDY"
   },
   "source": [
    "## –®–∞–≥ 4: –ë–µ–Ω—á–º–∞—Ä–∫\n",
    "\n",
    "–ß—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å —Ä–µ–∞–ª—å–Ω—ã–π –≤—ã–∏–≥—Ä—ã—à –æ—Ç —Å–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è, –Ω–∞–º –Ω—É–∂–Ω–æ —Å–∏–º—É–ª–∏—Ä–æ–≤–∞—Ç—å —Å–∏—Ç—É–∞—Ü–∏—é, –∫–æ–≥–¥–∞ `Target` –º–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ, —á–µ–º `Draft`. –í —Ä–µ–∞–ª—å–Ω–æ–π –∂–∏–∑–Ω–∏ —Ç–∞–∫ –∏ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç: `Draft` –º–æ–∂–µ—Ç –±—ã—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ 0.5B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∞ `Target` ‚Äî –Ω–∞ 70B.\n",
    "\n",
    "–ú—ã —Å–æ–∑–¥–∞–¥–∏–º –æ–±–µ—Ä—Ç–∫—É `HeavyTarget`, –∫–æ—Ç–æ—Ä–∞—è –±—É–¥–µ—Ç –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ –¥–æ–±–∞–≤–ª—è—Ç—å –∑–∞–¥–µ—Ä–∂–∫—É –ø–µ—Ä–µ–¥ –∫–∞–∂–¥—ã–º –≤—ã–∑–æ–≤–æ–º `forward`, –∏–º–∏—Ç–∏—Ä—É—è –º–µ–¥–ª–µ–Ω–Ω—ã–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –±–æ–ª—å—à–æ–π –º–æ–¥–µ–ª–∏. –ó–∞—Ç–µ–º –º—ã —Å—Ä–∞–≤–Ω–∏–º –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º (–∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–º) —Å–ø–æ—Å–æ–±–æ–º –∏ —Å –ø–æ–º–æ—â—å—é –Ω–∞—à–µ–≥–æ —Å–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞.–û–∂–∏–¥–∞–µ—Ç—Å—è, —á—Ç–æ —Å–ø–µ–∫—É–ª—è—Ç–∏–≤–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∂–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ (speedup ~ 1.5x+).\n",
    "\n",
    "–û–¥–Ω–∞–∫–æ, –æ–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –¥–ª—è –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Ä–∞–±–æ—Ç—ã –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö –Ω–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º KV-cache, —á—Ç–æ–±—ã –∑–∞–Ω–æ–≤–æ –Ω–µ –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞—Ç—å —É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è. –ò–º–µ–Ω–Ω–æ —Ç–∞–∫ speculative decoding —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –≤ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞—Ö."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "nMc3LzHtwOtY",
   "metadata": {
    "id": "nMc3LzHtwOtY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Benchmark...\n",
      "                 Prompt  Std (t/s)  Spec (t/s)  Speedup\n",
      "0  Write a Python funct       7.85       19.18     2.44\n",
      "1  The capital of Franc      10.85       11.51     1.06\n",
      "2  Explain the theory o      11.77       13.75     1.17\n"
     ]
    }
   ],
   "source": [
    "class HeavyTarget:\n",
    "    def __init__(self, model, delay=0.05):\n",
    "        self.model = model\n",
    "        self.delay = delay\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        time.sleep(self.delay)\n",
    "        return self.model(*args, **kwargs)\n",
    "    @property\n",
    "    def config(self): return self.model.config\n",
    "\n",
    "def autoregressive(model, text, max_new=50):\n",
    "    ids = tokenizer(text, return_tensors=\"pt\").to(device).input_ids\n",
    "    start = time.time()\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new):\n",
    "            out = model(ids)\n",
    "            tok = torch.argmax(out.logits[:, -1, :], dim=-1, keepdim=True)\n",
    "            ids = torch.cat([ids, tok], dim=1)\n",
    "            cnt += 1\n",
    "            if tok.item() == tokenizer.eos_token_id: break\n",
    "    return cnt, time.time() - start\n",
    "\n",
    "prompts = [\n",
    "    \"Write a Python function to calculate Fibonacci numbers.\",\n",
    "    \"The capital of France is\",\n",
    "    \"Explain the theory of relativity in simple terms.\"\n",
    "]\n",
    "heavy_target = HeavyTarget(target_model, 0.04)\n",
    "\n",
    "results = []\n",
    "print(\"Running Benchmark...\")\n",
    "for p in prompts:\n",
    "    # Std\n",
    "    s_tok, s_time = autoregressive(heavy_target, p)\n",
    "    s_speed = s_tok / s_time\n",
    "\n",
    "    # Spec\n",
    "    start = time.time()\n",
    "    _, stats = speculative_sampling(p, 50, heavy_target, draft_model, tokenizer)\n",
    "    spec_time = time.time() - start\n",
    "    spec_tok = stats['total_accepted'] + stats['target_calls']\n",
    "    spec_speed = spec_tok / spec_time\n",
    "\n",
    "    results.append({\n",
    "        \"Prompt\": p[:20],\n",
    "        \"Std (t/s)\": s_speed,\n",
    "        \"Spec (t/s)\": spec_speed,\n",
    "        \"Speedup\": spec_speed / s_speed\n",
    "    })\n",
    "\n",
    "print(pd.DataFrame(results).to_string(float_format=\"{:.2f}\".format))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bonus_section",
   "metadata": {
    "id": "bonus_section"
   },
   "source": [
    "## –ß—Ç–æ –¥–∞–ª—å—à–µ?\n",
    "\n",
    "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –æ–¥–Ω—É –∏–∑ —Å–ª–µ–¥—É—é—â–∏—Ö –∏–¥–µ–π:\n",
    "\n",
    "### 1. Quantized Draft Model (0.5 –±–∞–ª–ª–∞)\n",
    "–ú—ã —É—Å–∫–æ—Ä–∏–ª–∏ –º–æ–¥–µ–ª—å –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏. –ê –¥–∞–≤–∞–π—Ç–µ —Ç–µ–ø–µ—Ä—å —É—Å–∫–æ—Ä–∏–º Draft –º–æ–¥–µ–ª—å –∞–ø–ø–∞—Ä–∞—Ç–Ω–æ! –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∫–≤–∞–Ω—Ç–æ–≤–∞—Ç—å `NanoQwen` –≤ 4-–±–∏—Ç (–∏—Å–ø–æ–ª—å–∑—É—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ `bitsandbytes` –∏–ª–∏ `GPTQ`) –∏ –ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ, –∫–∞–∫ –∏–∑–º–µ–Ω–∏—Ç—Å—è –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥—Ä–∞—Ñ—Ç–æ–≤ –∏ –∏—Ç–æ–≥–æ–≤–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ (Speedup).\n",
    "\n",
    "### 2. LoRA Distillation (1 –±–∞–ª–ª)\n",
    "–ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ, —á—Ç–æ Draft –º–æ–¥–µ–ª—å –ø–ª–æ—Ö–æ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∞ —Å Target –º–æ–¥–µ–ª—å—é. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥–æ–æ–±—É—á–∏—Ç—å (Fine-Tune) Draft –º–æ–¥–µ–ª—å –Ω–∞ –Ω–∞–±–æ—Ä–µ –æ—Ç–≤–µ—Ç–æ–≤ Target –º–æ–¥–µ–ª–∏, –∏—Å–ø–æ–ª—å–∑—É—è LoRA (Low-Rank Adaptation). –î–∞–∂–µ 500 –ø—Ä–∏–º–µ—Ä–æ–≤ –∏ 10-15 –º–∏–Ω—É—Ç –æ–±—É—á–µ–Ω–∏—è –Ω–∞ T4 –º–æ–≥—É—Ç –ø–æ–≤—ã—Å–∏—Ç—å Acceptance Rate.\n",
    "\n",
    "### 3. Layer Pruning Distillation (1.5 –±–∞–ª–ª–∞)\n",
    "–í–æ–∑—å–º–∏—Ç–µ Target –º–æ–¥–µ–ª—å (1.5B) –∏ —Å–æ–∑–¥–∞–π—Ç–µ –∏–∑ –Ω–µ–µ Draft –º–æ–¥–µ–ª—å, –ø—Ä–æ—Å—Ç–æ —É–¥–∞–ª–∏–≤ –ø–æ–ª–æ–≤–∏–Ω—É —Å–ª–æ–µ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–∞–∂–¥—ã–π –≤—Ç–æ—Ä–æ–π). –ü–æ–ª—É—á–∏—Ç—Å—è \"–ø–æ–∫–∞–ª–µ—á–µ–Ω–Ω–∞—è\" –º–æ–¥–µ–ª—å ~0.8B. –ó–∞—Ç–µ–º –ø—Ä–æ–≤–µ–¥–∏—Ç–µ Knowledge Distillation (–æ–±—É—á–∏—Ç–µ –µ—ë –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –ª–æ–≥–∏—Ç—ã –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏) –∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–∞–∫ Draft –º–æ–¥–µ–ª—å."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d5b602-2a8d-4a19-886e-bf026a0bff51",
   "metadata": {},
   "source": [
    "#### 1. Quantized Draft Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68da6eee-b501-4783-b9c3-206e5b0c9d6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "draft_hf_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "    DRAFT_ID,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    attn_implementation=\"sdpa\",\n",
    ")\n",
    "draft_hf_4bit.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5fd3541b-c503-476f-98de-561f635ecc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogitsOnlyWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    def forward(self, input_ids):\n",
    "        return self.model(input_ids).logits\n",
    "\n",
    "draft_model_4bit = LogitsOnlyWrapper(draft_hf_4bit).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d96dade0-a317-4620-b47a-e9e1e8b1653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chat_prompt(user_text: str):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": user_text},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca3d89f9-d1cc-4a79-915f-1f69d570b880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draft_greedy_speed(draft_logits_model, prompt, max_new=64):\n",
    "    text = build_chat_prompt(prompt)\n",
    "    ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    t0 = time.time()\n",
    "    n = 0\n",
    "    \n",
    "    if device == \"cuda\":\n",
    "        with torch.no_grad(), torch.amp.autocast(\"cuda\", dtype=torch.float16):\n",
    "            for _ in range(max_new):\n",
    "                logits = draft_logits_model(ids)\n",
    "                tok = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "                ids = torch.cat([ids, tok], dim=1)\n",
    "                n += 1\n",
    "                if tok.item() == tokenizer.eos_token_id:\n",
    "                    break\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new):\n",
    "                logits = draft_logits_model(ids)\n",
    "                tok = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "                ids = torch.cat([ids, tok], dim=1)\n",
    "                n += 1\n",
    "                if tok.item() == tokenizer.eos_token_id:\n",
    "                    break\n",
    "                    \n",
    "    dt = time.time() - t0\n",
    "    return n, dt, (n / dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44215257-1814-4e60-a181-6f22b4c18796",
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_prompt = \"Explain speculative decoding in 3 bullet points\"\n",
    "n1, t1, s1 = draft_greedy_speed(draft_model, probe_prompt, max_new=64)\n",
    "n2, t2, s2 = draft_greedy_speed(draft_model_4bit, probe_prompt, max_new=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8fdd7590-96aa-4810-9b14-1222d55117fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Draft fp16 (NanoQwen): 22.18 tok/s  (tokens=64, time=2.89s)\n",
      "Draft 4bit (bnb): 3.94 tok/s  (tokens=64, time=16.24s)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Draft fp16 (NanoQwen): {s1:.2f} tok/s  (tokens={n1}, time={t1:.2f}s)\")\n",
    "print(f\"Draft 4bit (bnb): {s2:.2f} tok/s  (tokens={n2}, time={t2:.2f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "93fc924e-0a63-450a-94f3-d3506c733668",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Write a Python function to calculate Fibonacci numbers\",\n",
    "    \"The capital of France is\",\n",
    "    \"Explain the theory of relativity in simple terms\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dfd5ed-20b1-4839-8896-7a6354ffccc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_spec_bench(draft_logits_model, tag, K=5):\n",
    "    rows = []\n",
    "    for p in prompts:\n",
    "        start = time.time()\n",
    "        _, stats = speculative_sampling(p, 50, heavy_target, draft_logits_model, tokenizer, K=K)\n",
    "        dt = time.time() - start\n",
    "        spec_tok = stats[\"total_accepted\"] + stats[\"target_calls\"]\n",
    "        acc_rate = (stats[\"total_accepted\"] / max(1, stats[\"total_drafted\"]))\n",
    "        rows.append((tag, p[:22], dt, spec_tok/dt, acc_rate, stats[\"target_calls\"]))\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8b626981-a56a-4e3c-97ae-cab0ee3c1e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_fp16 = run_spec_bench(draft_model, \"draft_fp16_t1\", K=5)\n",
    "rows_4bit = run_spec_bench(draft_model_4bit, \"draft_4bit\", K=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f7c44d63-6510-4364-a949-53ffa3d6ff83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Draft</th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Time(s)</th>\n",
       "      <th>Spec tok/s</th>\n",
       "      <th>AcceptRate</th>\n",
       "      <th>TargetCalls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>draft_fp16_t1</td>\n",
       "      <td>Write a Python functio</td>\n",
       "      <td>2.803057</td>\n",
       "      <td>17.837665</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>draft_fp16_t1</td>\n",
       "      <td>The capital of France</td>\n",
       "      <td>4.592845</td>\n",
       "      <td>11.757418</td>\n",
       "      <td>0.493506</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>draft_fp16_t1</td>\n",
       "      <td>Explain the theory of</td>\n",
       "      <td>3.894068</td>\n",
       "      <td>13.096843</td>\n",
       "      <td>0.584615</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>draft_4bit</td>\n",
       "      <td>Write a Python functio</td>\n",
       "      <td>5.910618</td>\n",
       "      <td>8.459353</td>\n",
       "      <td>0.709091</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>draft_4bit</td>\n",
       "      <td>The capital of France</td>\n",
       "      <td>1.098486</td>\n",
       "      <td>1.820687</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>draft_4bit</td>\n",
       "      <td>Explain the theory of</td>\n",
       "      <td>6.372203</td>\n",
       "      <td>8.003511</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Draft                  Prompt   Time(s)  Spec tok/s  AcceptRate  \\\n",
       "0  draft_fp16_t1  Write a Python functio  2.803057   17.837665    0.911111   \n",
       "1  draft_fp16_t1  The capital of France   4.592845   11.757418    0.493506   \n",
       "2  draft_fp16_t1  Explain the theory of   3.894068   13.096843    0.584615   \n",
       "3     draft_4bit  Write a Python functio  5.910618    8.459353    0.709091   \n",
       "4     draft_4bit  The capital of France   1.098486    1.820687    0.000000   \n",
       "5     draft_4bit  Explain the theory of   6.372203    8.003511    0.727273   \n",
       "\n",
       "   TargetCalls  \n",
       "0            9  \n",
       "1           16  \n",
       "2           13  \n",
       "3           11  \n",
       "4            2  \n",
       "5           11  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t1 = pd.DataFrame(rows_fp16 + rows_4bit, columns=[\"Draft\",\"Prompt\",\"Time(s)\",\"Spec tok/s\",\"AcceptRate\",\"TargetCalls\"])\n",
    "df_t1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1cbf43-a95a-4638-a2e2-528eac1fe10a",
   "metadata": {},
   "source": [
    "#### 2. LoRA Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c93824e9-dde2-420f-a38f-bcdb593a202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompts(n=500):\n",
    "    topics = [\"math\", \"python\", \"history\", \"biology\", \"economics\", \"literature\", \"physics\", \"travel\", \"cooking\", \"ml\"]\n",
    "    tasks = [\n",
    "        \"Explain in simple terms:\",\n",
    "        \"Write a short step-by-step guide for:\",\n",
    "        \"Give 5 bullet points about:\",\n",
    "        \"Provide a concise definition of:\",\n",
    "        \"Answer with an example:\",\n",
    "    ]\n",
    "    prompts = []\n",
    "    for _ in range(n):\n",
    "        a = np.random.choice(tasks)\n",
    "        t = np.random.choice(topics)\n",
    "        prompts.append(f\"{a} {t}\")\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f3b6c676-1593-4b53-842c-77339491148c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def teacher_generate_answer(prompt, max_new=128):\n",
    "    prompt_text = build_chat_prompt(prompt)\n",
    "    inp = tokenizer(prompt_text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out = target_model.generate(\n",
    "            **inp,\n",
    "            do_sample=False,\n",
    "            temperature=None,       \n",
    "            top_p=None,               \n",
    "            top_k=None, \n",
    "            max_new_tokens=max_new,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    gen_ids = out[0, inp.input_ids.shape[1]:]\n",
    "    return tokenizer.decode(gen_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "82a243ab-e603-465f-b57c-6fd0fa37142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EX = 500\n",
    "prompts_lora = make_prompts(N_EX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dec0ee12-4ccb-4d3e-bd2b-7a5842cb364b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 100/500\n",
      "done 200/500\n",
      "done 300/500\n",
      "done 400/500\n",
      "done 500/500\n"
     ]
    }
   ],
   "source": [
    "pairs = []\n",
    "for i, p in enumerate(prompts_lora, 1):\n",
    "    ans = teacher_generate_answer(p, max_new=96)\n",
    "    pairs.append({\"prompt\": p, \"answer\": ans})\n",
    "    if i % 100 == 0:\n",
    "        print(f\"done {i}/{N_EX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2bcf537f-6239-4303-9f38-4fb154215b55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draft_hf = AutoModelForCausalLM.from_pretrained(\n",
    "    DRAFT_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    attn_implementation=\"sdpa\",\n",
    ").to(device)\n",
    "draft_hf.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b6dc3126-f5eb-46c3-aa25-e2e4885ceeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_cfg = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.04,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "768fbf5c-2300-4d02-b1d5-9ea1c8b92c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,798,208 || all params: 502,830,976 || trainable%: 1.7497\n"
     ]
    }
   ],
   "source": [
    "draft_lora = get_peft_model(draft_hf, lora_cfg)\n",
    "draft_lora.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6d3f22fa-a582-402d-b221-df7c79f8664c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillSFTDataset(Dataset):\n",
    "    def __init__(self, pairs, max_len=512):\n",
    "        self.pairs = pairs\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.pairs[idx][\"prompt\"]\n",
    "        a = self.pairs[idx][\"answer\"]\n",
    "\n",
    "        prompt_text = build_chat_prompt(p)\n",
    "        full_text = prompt_text + a\n",
    "\n",
    "        prompt_ids = tokenizer(prompt_text, return_tensors=\"pt\", add_special_tokens=False).input_ids[0]\n",
    "        full_ids = tokenizer(full_text, return_tensors=\"pt\", add_special_tokens=False).input_ids[0]\n",
    "\n",
    "        full_ids = full_ids[: self.max_len]\n",
    "        prompt_len = min(len(prompt_ids), len(full_ids))\n",
    "\n",
    "        labels = full_ids.clone()\n",
    "        labels[:prompt_len] = -100\n",
    "\n",
    "        return {\"input_ids\": full_ids, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eaf3e136-5003-460c-85b4-cf8cf5075dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    pad_id = tokenizer.eos_token_id if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n",
    "    maxlen = max(x[\"input_ids\"].shape[0] for x in batch)\n",
    "\n",
    "    input_ids = torch.full((len(batch), maxlen), pad_id, dtype=torch.long)\n",
    "    labels = torch.full((len(batch), maxlen), -100, dtype=torch.long)\n",
    "\n",
    "    for i, ex in enumerate(batch):\n",
    "        l = ex[\"input_ids\"].shape[0]\n",
    "        input_ids[i, :l] = ex[\"input_ids\"]\n",
    "        labels[i, :l] = ex[\"labels\"]\n",
    "\n",
    "    return {\"input_ids\": input_ids.to(device), \"labels\": labels.to(device)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eca82458-dfc5-4ab1-8794-6f1ee2b0fcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DistillSFTDataset(pairs, max_len=512)\n",
    "dl = DataLoader(ds, batch_size=1, shuffle=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f0238455-78e4-4195-bc9b-5b07484b69de",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.AdamW(draft_lora.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "67060760-4f85-489f-9cb6-043dc7aef762",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_accum = 10\n",
    "max_steps = 500\n",
    "scaler = torch.amp.GradScaler(\"cuda\", enabled=(device == \"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7cc50d32-c8af-44e7-872a-ce17924cd82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "draft_lora.train()\n",
    "optim.zero_grad(set_to_none=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "419c171d-4434-4483-9254-9c391c3e6a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100/500 | loss=0.5005 | elapsed=30.8s\n",
      "step 200/500 | loss=0.4576 | elapsed=59.8s\n",
      "step 300/500 | loss=0.0274 | elapsed=88.9s\n",
      "step 400/500 | loss=0.0031 | elapsed=118.1s\n",
      "step 500/500 | loss=0.0065 | elapsed=148.6s\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "t0 = time.time()\n",
    "\n",
    "for batch in dl:\n",
    "    if device == \"cuda\":\n",
    "        with torch.amp.autocast(\"cuda\", dtype=torch.float16):\n",
    "            out = draft_lora(**batch)\n",
    "            loss = out.loss / grad_accum\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % grad_accum == 0:\n",
    "            scaler.step(optim)\n",
    "            scaler.update()\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "    else:\n",
    "        out = draft_lora(**batch)\n",
    "        loss = out.loss / grad_accum\n",
    "        loss.backward()\n",
    "\n",
    "        if (step + 1) % grad_accum == 0:\n",
    "            optim.step()\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "\n",
    "    if (step + 1) % 100 == 0:\n",
    "        print(f\"step {step+1}/{max_steps} | loss={loss.item()*grad_accum:.4f} | elapsed={time.time()-t0:.1f}s\")\n",
    "\n",
    "    step += 1\n",
    "    if step >= max_steps:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "700b2280-9e31-43aa-8e81-8ec3c0a9481e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen2ForCausalLM(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(151936, 896)\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2SdpaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=896, out_features=896, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.04, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=896, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=896, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=896, out_features=128, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.04, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=896, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=896, out_features=128, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.04, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=896, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=896, out_features=896, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.04, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=896, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=896, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): Qwen2RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=896, out_features=4864, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.04, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=896, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4864, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=896, out_features=4864, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.04, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=896, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4864, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4864, out_features=896, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.04, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4864, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=896, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draft_lora.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "98718823-dd08-43b7-8183-ff18b9c49c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "draft_lora_logits = LogitsOnlyWrapper(draft_lora).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0e611e32-fed3-450f-a248-d67c58088ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompts = [\n",
    "    \"Explain what RMSNorm is.\",\n",
    "    \"Give a short code example of a Python decorator.\",\n",
    "    \"Why does RoPE help with long context?\",\n",
    "    \"Write 3 tips for debugging PyTorch models.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c3fbe93c-e5bd-47da-8bb5-b1ec1121713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acceptance_rate_on(prompts, draft_logits_model, K=5, max_new=64):\n",
    "    acc = []\n",
    "    for p in prompts:\n",
    "        _, stats = speculative_sampling(p, max_new, target_model, draft_logits_model, tokenizer, K=K)\n",
    "        acc.append(stats[\"total_accepted\"] / max(1, stats[\"total_drafted\"]))\n",
    "    return sum(acc) / len(acc), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0b7efc88-2788-4f50-a21e-58d741b77be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_mean, base_list = acceptance_rate_on(eval_prompts, draft_model, K=5, max_new=64)\n",
    "lora_mean, lora_list = acceptance_rate_on(eval_prompts, draft_lora_logits, K=5, max_new=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "4eae39f7-a215-4db3-970f-8afcfb97f9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline NanoQwen AR: 0.571549, [0.44, 0.8153846153846154, 0.41904761904761906, 0.611764705882353]\n",
      "LoRA Draft AR: 0.581818, [0.42727272727272725, 0.9, 0.2714285714285714, 0.7285714285714285]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Baseline NanoQwen AR: {base_mean:6f}, {base_list}\")\n",
    "print(f\"LoRA Draft AR: {lora_mean:6f}, {lora_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5616a7-b914-4a4d-85fc-fa465a4d88de",
   "metadata": {},
   "source": [
    "#### 3. Layer Pruning Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "82c81a16-2044-4dfb-8120-6f1cc07caa21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 1536)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher = target_model\n",
    "teacher.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c0c3273f-4ebb-4d79-9937-bd21b0feec47",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_cfg = copy.deepcopy(teacher.config)\n",
    "student_cfg.num_hidden_layers = teacher.config.num_hidden_layers // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9a679f6b-f856-4660-b1f0-06d9df326dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 1536)\n",
       "    (layers): ModuleList(\n",
       "      (0-13): 14 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student = AutoModelForCausalLM.from_config(student_cfg).to(device).half()\n",
    "student.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8fc872bd-2880-49d2-b178-354d8c683ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sd = teacher.state_dict()\n",
    "s_sd = student.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a8ce9b3b-a00e-4a9a-b6b8-dd41ba60fbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_layer_prefix(sd_keys):\n",
    "    if any(k.startswith(\"model.layers.\") for k in sd_keys):\n",
    "        return \"model.layers.\"\n",
    "    if any(k.startswith(\"layers.\") for k in sd_keys):\n",
    "        return \"layers.\"\n",
    "    raise ValueError(\"err detect layer prefix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "138623cb-7458-45f5-aca3-e8a36de0adf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_pref = detect_layer_prefix(t_sd.keys())\n",
    "s_pref = detect_layer_prefix(s_sd.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "26bbd38c-094f-4f83-869e-ddabf8c8ae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sd = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2a7c8b5b-6ad0-4d22-8b8c-069535080a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in t_sd.items():\n",
    "    if k.startswith(t_pref):\n",
    "        continue\n",
    "    if k in s_sd and s_sd[k].shape == v.shape:\n",
    "        new_sd[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "84d41d93-83a8-4cd1-b654-10e3ebacaf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_n = student_cfg.num_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "acc72bf8-2516-42cd-96fb-22bc3b2463e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_s in range(s_n):\n",
    "    i_t = i_s * 2\n",
    "    for k_s in list(s_sd.keys()):\n",
    "        if not k_s.startswith(f\"{s_pref}{i_s}.\"):\n",
    "            continue\n",
    "        k_tail = k_s.split(f\"{s_pref}{i_s}.\", 1)[1]\n",
    "        k_t = f\"{t_pref}{i_t}.\" + k_tail\n",
    "        if k_t in t_sd and s_sd[k_s].shape == t_sd[k_t].shape:\n",
    "            new_sd[k_s] = t_sd[k_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "78e5f96b-ad71-42fe-b877-d66404f8c7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: 0 Unexpected: 0\n"
     ]
    }
   ],
   "source": [
    "missing, unexpected = student.load_state_dict(new_sd, strict=False)\n",
    "print(\"Missing:\", len(missing), \"Unexpected:\", len(unexpected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5d826098-5403-421c-b64d-5df37070fa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "student.train()\n",
    "lora_kd_cfg = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5f3b55f4-d0df-4aa4-ba7c-2db31bcaf6ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen2ForCausalLM(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(151936, 1536)\n",
       "        (layers): ModuleList(\n",
       "          (0-13): 14 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2SdpaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): Qwen2RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8960, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8960, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8960, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_lora = get_peft_model(student, lora_kd_cfg).to(device)\n",
    "student_lora.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a40b0236-19d5-47d3-9496-3fed5a574995",
   "metadata": {},
   "outputs": [],
   "source": [
    "kd_prompts = make_prompts(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "73338d19-960a-475e-b012-86b21c9dfa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KDPromptDataset(Dataset):\n",
    "    def __init__(self, prompts, max_len=256):\n",
    "        self.prompts = prompts\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.prompts[idx]\n",
    "        txt = build_chat_prompt(p)\n",
    "        ids = tokenizer(txt, return_tensors=\"pt\", add_special_tokens=False).input_ids[0][:self.max_len]\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3c7b7684-ed75-41cb-ac7b-831cfc4c748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kd_collate(batch):\n",
    "    pad_id = tokenizer.eos_token_id if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n",
    "    maxlen = max(x.shape[0] for x in batch)\n",
    "\n",
    "    input_ids = torch.full((len(batch), maxlen), pad_id, dtype=torch.long)\n",
    "    attn = torch.zeros((len(batch), maxlen), dtype=torch.long)\n",
    "\n",
    "    for i, ids in enumerate(batch):\n",
    "        l = ids.shape[0]\n",
    "        input_ids[i, :l] = ids\n",
    "        attn[i, :l] = 1\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids.to(device),\n",
    "        \"attention_mask\": attn.to(device),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f6531ea9-dab7-4621-b1b1-c6ad6846ed36",
   "metadata": {},
   "outputs": [],
   "source": [
    "kd_ds = KDPromptDataset(kd_prompts, max_len=256)\n",
    "kd_dl = DataLoader(kd_ds, batch_size=1, shuffle=True, collate_fn=kd_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b9f13dd9-416c-4f29-b16c-ef75d6f41f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kd_loss(student_logits, teacher_logits, attention_mask, T=2.0):\n",
    "    s = student_logits / T\n",
    "    t = teacher_logits / T\n",
    "    logp_s = torch.log_softmax(s, dim=-1)\n",
    "    p_t = torch.softmax(t, dim=-1)\n",
    "\n",
    "    kl = torch.sum(p_t * (torch.log(p_t + 1e-8) - logp_s), dim=-1)\n",
    "    kl = kl * attention_mask\n",
    "    return kl.sum() / attention_mask.sum() * (T * T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e6d893b5-b9c4-4a0e-b23c-dba444708086",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.AdamW(student_lora.parameters(), lr=2e-4)\n",
    "scaler = torch.amp.GradScaler(\"cuda\", enabled=(device == \"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7e6b45d4-17aa-498b-823a-7bbdb4be9b7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen2ForCausalLM(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(151936, 1536)\n",
       "        (layers): ModuleList(\n",
       "          (0-13): 14 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2SdpaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): Qwen2RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8960, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8960, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8960, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_steps = 500\n",
    "t0 = time.time()\n",
    "\n",
    "teacher.eval()\n",
    "student_lora.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "05ee3005-cda9-4f06-8c00-c94b48dc4183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100/500 | kd_loss=1.3122 | elapsed=24.6s\n",
      "step 200/500 | kd_loss=0.5965 | elapsed=49.1s\n",
      "step 300/500 | kd_loss=0.3884 | elapsed=73.3s\n",
      "step 400/500 | kd_loss=0.4853 | elapsed=97.9s\n",
      "step 500/500 | kd_loss=0.2027 | elapsed=121.8s\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(kd_dl, 1):\n",
    "    if device == \"cuda\":\n",
    "        with torch.no_grad(), torch.amp.autocast(\"cuda\", dtype=torch.float16):\n",
    "            t_out = teacher(**batch)\n",
    "            t_logits = t_out.logits.detach()\n",
    "\n",
    "        with torch.amp.autocast(\"cuda\", dtype=torch.float16):\n",
    "            s_out = student_lora(**batch)\n",
    "            s_logits = s_out.logits\n",
    "            loss = kd_loss(s_logits, t_logits, batch[\"attention_mask\"], T=2.0)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optim)\n",
    "        scaler.update()\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            t_out = teacher(**batch)\n",
    "            t_logits = t_out.logits.detach()\n",
    "\n",
    "        s_out = student_lora(**batch)\n",
    "        s_logits = s_out.logits\n",
    "        loss = kd_loss(s_logits, t_logits, batch[\"attention_mask\"], T=2.0)\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(f\"step {step}/{max_steps} | kd_loss={loss.item():.4f} | elapsed={time.time()-t0:.1f}s\")\n",
    "\n",
    "    if step >= max_steps:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f8bde0c7-6251-46eb-b81f-605efec24965",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen2ForCausalLM(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(151936, 1536)\n",
       "        (layers): ModuleList(\n",
       "          (0-13): 14 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2SdpaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): Qwen2RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8960, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8960, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8960, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_lora.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e8689a5c-fdde-458d-8b0d-b98b111db931",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_kd_draft = LogitsOnlyWrapper(student_lora).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4ed8979c-c406-4411-a9c7-99126b698602",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_mean, _ = acceptance_rate_on(eval_prompts, draft_model, K=5, max_new=64)\n",
    "pruned_mean, _ = acceptance_rate_on(eval_prompts, pruned_kd_draft, K=5, max_new=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "cf35f825-f8fe-4f26-8f72-59a908558d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline NanoQwen AR: 0.571549\n",
      "Pruned+KD Draft AR: 0.021690\n"
     ]
    }
   ],
   "source": [
    "print(f\"Baseline NanoQwen AR: {base_mean:6f}\")\n",
    "print(f\"Pruned+KD Draft AR: {pruned_mean:6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "24c5db16-520c-444a-a74c-80168f99cae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Draft</th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Time(s)</th>\n",
       "      <th>Spec tok/s</th>\n",
       "      <th>AcceptRate</th>\n",
       "      <th>TargetCalls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>draft_prunedKD</td>\n",
       "      <td>Write a Python functio</td>\n",
       "      <td>18.314411</td>\n",
       "      <td>2.730090</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>draft_prunedKD</td>\n",
       "      <td>The capital of France</td>\n",
       "      <td>0.793908</td>\n",
       "      <td>2.519183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>draft_prunedKD</td>\n",
       "      <td>Explain the theory of</td>\n",
       "      <td>17.374268</td>\n",
       "      <td>2.877819</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Draft                  Prompt    Time(s)  Spec tok/s  AcceptRate  \\\n",
       "0  draft_prunedKD  Write a Python functio  18.314411    2.730090    0.008333   \n",
       "1  draft_prunedKD  The capital of France    0.793908    2.519183    0.000000   \n",
       "2  draft_prunedKD  Explain the theory of   17.374268    2.877819    0.022222   \n",
       "\n",
       "   TargetCalls  \n",
       "0           48  \n",
       "1            2  \n",
       "2           45  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_pruned = run_spec_bench(pruned_kd_draft, \"draft_prunedKD\", K=5)\n",
    "df_t3 = pd.DataFrame(rows_pruned, columns=[\"Draft\",\"Prompt\",\"Time(s)\",\"Spec tok/s\",\"AcceptRate\",\"TargetCalls\"])\n",
    "df_t3"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
