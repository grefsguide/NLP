{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hmqAs1bbR-sq",
   "metadata": {
    "id": "hmqAs1bbR-sq"
   },
   "source": [
    "# Retrieval-Augmented Generation (RAG)\n",
    " \n",
    "**–¢–µ–º–∞:** –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ, –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏ –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ RAG-—Å–∏—Å—Ç–µ–º.\n",
    "\n",
    "## –í–≤–µ–¥–µ–Ω–∏–µ\n",
    "–í —Ä–∞–º–∫–∞—Ö –¥–∞–Ω–Ω–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è –≤–∞–º –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç –ø—Ä–æ–π—Ç–∏ –ø–æ–ª–Ω—ã–π –ø—É—Ç—å ML-–∏–Ω–∂–µ–Ω–µ—Ä–∞ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å RAG: –æ—Ç —Å–±–æ—Ä–∫–∏ –±–∞–∑–æ–≤–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –¥–æ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞ –∏ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞. –ú—ã –±—É–¥–µ–º —Ä–∞–±–æ—Ç–∞—Ç—å —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º **SciFact** (–ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞—É—á–Ω—ã—Ö —Ñ–∞–∫—Ç–æ–≤).\n",
    "\n",
    "### –ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞\n",
    "–í–∞–∂–Ω–æ —Å–æ–±–ª—é–¥–∞—Ç—å –≥–∏–≥–∏–µ–Ω—É ML-—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤. –ú—ã —Ä–∞–∑–¥–µ–ª–∏–ª–∏ –¥–∞–Ω–Ω—ã–µ –Ω–∞ –¥–≤–∞ —Å–µ—Ç–∞:\n",
    "1.  **Main Split (200 –∑–∞–ø—Ä–æ—Å–æ–≤):** –í–∞—à–∞ \"Dev\" –≤—ã–±–æ—Ä–∫–∞. –í—Å–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –ø—Ä–æ–≥–æ–Ω—ã, –ø–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (chunk size, top_k) –∏ –æ—Ç–ª–∞–¥–∫—É –≤—ã –¥–µ–ª–∞–µ—Ç–µ **—Ç–æ–ª—å–∫–æ** –Ω–∞ –Ω–µ–π. –ú—ã —Ö–æ—Ç–∏–º –∏–∑–±–µ–∂–∞—Ç—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ.\n",
    "2.  **Challenge Split (50 –∑–∞–ø—Ä–æ—Å–æ–≤):** –í–∞—à–∞ \"Test\" –≤—ã–±–æ—Ä–∫–∞. –í—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ –µ—ë **—Ä–æ–≤–Ω–æ –æ–¥–∏–Ω —Ä–∞–∑** –≤ —Å–∞–º–æ–º –∫–æ–Ω—Ü–µ –Ω–æ—É—Ç–±—É–∫–∞ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –ª—É—á—à–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bIDpeqtHR-sr",
   "metadata": {
    "id": "bIDpeqtHR-sr"
   },
   "source": [
    "## 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è\n",
    "–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π. –ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º `LlamaIndex` –∫–∞–∫ –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä, `Qdrant` –∫–∞–∫ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î –∏ `HuggingFace` –¥–ª—è –º–æ–¥–µ–ª–µ–π."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bW1zqb1R-ss",
   "metadata": {
    "id": "6bW1zqb1R-ss"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install llama-index-core llama-index-llms-huggingface llama-index-embeddings-huggingface llama-index-vector-stores-qdrant llama-index-retrievers-bm25\n",
    "!pip install qdrant-client ir_datasets ir_measures bitsandbytes accelerate transformers\n",
    "!pip install pandas matplotlib seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c71ca44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX configured\n"
     ]
    }
   ],
   "source": [
    "# JAX Memory Configuration (CRITICAL: must run before imports)\n",
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n",
    "print(\"JAX configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "QbsfjMMqR-ss",
   "metadata": {
    "id": "QbsfjMMqR-ss"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import gc\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Optimizations for T4 GPU in Colab\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "ARTIFACTS_DIR = os.path.abspath(\"./artifacts\")\n",
    "QDRANT_PATH = os.path.join(ARTIFACTS_DIR, \"qdrant_local\")\n",
    "LOGS_DIR = os.path.join(ARTIFACTS_DIR, \"logs\")\n",
    "\n",
    "os.makedirs(QDRANT_PATH, exist_ok=True)\n",
    "os.makedirs(LOGS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oRf9meVTR-ss",
   "metadata": {
    "id": "oRf9meVTR-ss"
   },
   "source": [
    "## 2. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö (SciFact)\n",
    "–ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ—Ä–ø—É—Å –∏ –∑–∞–ø—Ä–æ—Å—ã. SciFact ‚Äî —ç—Ç–æ —Å–ª–æ–∂–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç, –≥–¥–µ –∑–∞–ø—Ä–æ—Å —á–∞—Å—Ç–æ —Ç—Ä–µ–±—É–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å–≤—è–∑–µ–π –≤ –Ω–∞—É—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "VXWO8grrR-ss",
   "metadata": {
    "id": "VXWO8grrR-ss"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corpus...\n",
      "Loading queries...\n",
      "Loading qrels...\n",
      "Corpus size: 5183\n",
      "Main split: 200 queries, Challenge split: 50 queries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grigorii\\AppData\\Local\\Temp\\ipykernel_40136\\2705820492.py:19: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  qrels_map = df_qrels.groupby(\"query_id\").apply(lambda x: dict(zip(x[\"corpus_id\"], x[\"score\"]))).to_dict()\n"
     ]
    }
   ],
   "source": [
    "import ir_datasets\n",
    "\n",
    "ds_corpus = ir_datasets.load(\"beir/scifact\")\n",
    "ds_test = ir_datasets.load(\"beir/scifact/test\")\n",
    "\n",
    "print(\"Loading corpus...\")\n",
    "corpus_rows = [{\"doc_id\": str(d.doc_id), \"title\": d.title or \"\", \"text\": d.text or \"\"} for d in ds_corpus.docs_iter()]\n",
    "df_corpus = pd.DataFrame(corpus_rows)\n",
    "\n",
    "print(\"Loading queries...\")\n",
    "query_rows = [{\"query_id\": str(q.query_id), \"text\": q.text} for q in ds_test.queries_iter()]\n",
    "df_queries = pd.DataFrame(query_rows)\n",
    "\n",
    "print(\"Loading qrels...\")\n",
    "qrel_rows = [{\"query_id\": str(q.query_id), \"corpus_id\": str(q.doc_id), \"score\": int(q.relevance)} for q in ds_test.qrels_iter()]\n",
    "df_qrels = pd.DataFrame(qrel_rows)\n",
    "\n",
    "# Map for metrics calculation\n",
    "qrels_map = df_qrels.groupby(\"query_id\").apply(lambda x: dict(zip(x[\"corpus_id\"], x[\"score\"]))).to_dict()\n",
    "\n",
    "# Fixed Splits\n",
    "all_qids = df_queries[\"query_id\"].unique()\n",
    "rng = np.random.default_rng(42)\n",
    "rng.shuffle(all_qids)\n",
    "split_main = all_qids[:200]\n",
    "split_challenge = all_qids[200:250]\n",
    "\n",
    "print(f\"Corpus size: {len(df_corpus)}\")\n",
    "print(f\"Main split: {len(split_main)} queries, Challenge split: {len(split_challenge)} queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kMIVdIjxR-ss",
   "metadata": {
    "id": "kMIVdIjxR-ss"
   },
   "source": [
    "## 3. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏ –ú–æ–¥–µ–ª–∏\n",
    "–ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º `dataclasses` –¥–ª—è —Å—Ç—Ä–æ–≥–æ–π —Ç–∏–ø–∏–∑–∞—Ü–∏–∏ –∫–æ–Ω—Ñ–∏–≥–æ–≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2j0WiejlR-ss",
   "metadata": {
    "id": "2j0WiejlR-ss"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Soft\\Anaconda\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "resource module not available on Windows\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, Document, Settings, StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core.node_parser import SentenceSplitter, HierarchicalNodeParser\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from qdrant_client import QdrantClient\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# --- CONFIG CLASSES ---\n",
    "@dataclass\n",
    "class ChunkingConfig:\n",
    "    chunk_size: int = 512\n",
    "    chunk_overlap: int = 100\n",
    "    use_hierarchical: bool = False\n",
    "    child_chunk_size: int = 128\n",
    "\n",
    "@dataclass\n",
    "class RetrievalConfig:\n",
    "    top_k: int = 5          # Final k documents for LLM\n",
    "    overfetch_k: int = 15   # Candidates fetched from DB (The \"Funnel\" top)\n",
    "    mode: str = \"dense\"     # \"dense\" | \"hybrid\"\n",
    "    use_reranker: bool = False\n",
    "    rerank_top_n: int = 5   # Documents to keep after reranking\n",
    "\n",
    "@dataclass\n",
    "class LLMConfig:\n",
    "    model_name: str = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "    max_new_tokens: int = 256\n",
    "    context_window: int = 2048\n",
    "    load_in_4bit: bool = True\n",
    "    # Subsample for E2E evaluation speed\n",
    "    e2e_eval_n: int = 10\n",
    "    prompt_template: str = \"Context:\\n{context_str}\\n\\nQuery: {query_str}\\nAnswer:\"\n",
    "\n",
    "@dataclass\n",
    "class EmbeddingConfig:\n",
    "    model_name: str = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "    truncate_dim: Optional[int] = None # For Matryoshka learning\n",
    "\n",
    "@dataclass\n",
    "class RAGConfig:\n",
    "    name: str = \"baseline\"\n",
    "    chunking: ChunkingConfig = field(default_factory=ChunkingConfig)\n",
    "    retrieval: RetrievalConfig = field(default_factory=RetrievalConfig)\n",
    "    llm: LLMConfig = field(default_factory=LLMConfig)\n",
    "    embedding: EmbeddingConfig = field(default_factory=EmbeddingConfig)\n",
    "    qdrant_collection: str = \"scifact_dense_base\"\n",
    "    recreate_collection: bool = False\n",
    "    rerank_model: str = \"Qwen/Qwen3-Reranker-0.6B\"\n",
    "\n",
    "# --- MODEL HELPERS ---\n",
    "_CACHED_LLM = None\n",
    "_CACHED_EMBED = None\n",
    "\n",
    "def unload_embedder():\n",
    "    global _CACHED_EMBED\n",
    "    if _CACHED_EMBED:\n",
    "        del _CACHED_EMBED\n",
    "        _CACHED_EMBED = None\n",
    "        Settings.embed_model = None\n",
    "        gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "def get_embedder(cfg: EmbeddingConfig):\n",
    "    global _CACHED_EMBED\n",
    "    if _CACHED_EMBED: return _CACHED_EMBED\n",
    "    _CACHED_EMBED = HuggingFaceEmbedding(\n",
    "        model_name=cfg.model_name,\n",
    "        device=\"cuda\",\n",
    "        normalize=True,\n",
    "        truncate_dim=cfg.truncate_dim,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    return _CACHED_EMBED\n",
    "\n",
    "def unload_llm():\n",
    "    global _CACHED_LLM\n",
    "    if _CACHED_LLM is not None:\n",
    "        try:\n",
    "            del _CACHED_LLM\n",
    "        except Exception:\n",
    "            pass\n",
    "        _CACHED_LLM = None\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "def get_llm(cfg: LLMConfig):\n",
    "    global _CACHED_LLM\n",
    "    if _CACHED_LLM: return _CACHED_LLM\n",
    "    from transformers import AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "    bnb = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "    )\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(cfg.model_name, trust_remote_code=True)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "\n",
    "    def completion_to_prompt(prompt: str) -> str:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a careful scientific assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "        return tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    _CACHED_LLM = HuggingFaceLLM(\n",
    "        model_name=cfg.model_name,\n",
    "        context_window=cfg.context_window,\n",
    "        max_new_tokens=cfg.max_new_tokens,\n",
    "        model_kwargs={\n",
    "            \"quantization_config\": bnb,\n",
    "            \"trust_remote_code\": True,\n",
    "        },\n",
    "        tokenizer_name=cfg.model_name,\n",
    "        tokenizer_kwargs={\"trust_remote_code\": True},\n",
    "        completion_to_prompt=completion_to_prompt,\n",
    "        generate_kwargs={\n",
    "            \"do_sample\": False,\n",
    "            \"temperature\": 0.0,\n",
    "            \"repetition_penalty\": 1.05,\n",
    "            \"pad_token_id\": tok.pad_token_id,\n",
    "            \"eos_token_id\": tok.eos_token_id,\n",
    "        },\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    return _CACHED_LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o4jfiHu9R-st",
   "metadata": {
    "id": "o4jfiHu9R-st"
   },
   "source": [
    "## 4. –°–∏—Å—Ç–µ–º–∞ –ú–µ—Ç—Ä–∏–∫ (Metrics System)\n",
    "\n",
    "–î–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–≤—É—Ö—É—Ä–æ–≤–Ω–µ–≤—É—é —Å–∏—Å—Ç–µ–º—É –º–µ—Ç—Ä–∏–∫.\n",
    "\n",
    "### 1. Retrieval Phase (–ü–æ–∏—Å–∫)\n",
    "–û—Ü–µ–Ω–∏–≤–∞–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∫—É—Å–∫–∏ —Ç–µ–∫—Å—Ç–∞ (chunks) –º—ã –Ω–∞—à–ª–∏ –≤ –±–∞–∑–µ.\n",
    "\n",
    "**A. –° —É—á–∏—Ç–µ–ª–µ–º (Supervised / Reference-based):**\n",
    "–¢—Ä–µ–±—É—é—Ç –Ω–∞–ª–∏—á–∏—è \"–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤\" (qrels).\n",
    "*   **`nDCG@k`**: –û—Å–Ω–æ–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è. –£—á–∏—Ç—ã–≤–∞–µ—Ç –Ω–µ —Ç–æ–ª—å–∫–æ —Ñ–∞–∫—Ç –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞, –Ω–æ –∏ –µ–≥–æ –ø–æ–∑–∏—Ü–∏—é (—á–µ–º –≤—ã—à–µ, —Ç–µ–º –ª—É—á—à–µ).\n",
    "*   **`Recall@k`**: –ü–æ–ª–Ω–æ—Ç–∞. –ö–∞–∫—É—é –¥–æ–ª—é –≤—Å–µ—Ö —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –º—ã –Ω–∞—à–ª–∏ –≤ —Ç–æ–ø-k.\n",
    "\n",
    "**B. –ë–µ–∑ —É—á–∏—Ç–µ–ª—è (Unsupervised / Reference-free):**\n",
    "–ü–æ–ª–µ–∑–Ω—ã –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ, –≥–¥–µ –Ω–µ—Ç —Ä–∞–∑–º–µ—Ç–∫–∏.\n",
    "*   **`Mean Relevance Score`**: –°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Å–∫–æ—Ä–∞ (similarity), –∫–æ—Ç–æ—Ä—ã–π –≤—ã–¥–∞–µ—Ç —Ä–µ—Ç—Ä–∏–≤–µ—Ä. –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç \"—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å\" –º–æ–¥–µ–ª–∏ –≤ –Ω–∞–π–¥–µ–Ω–Ω–æ–º.\n",
    "*   **`Redundancy`**: –ò–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å. –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏ –ø–æ—Ö–æ–∂–∏ –¥—Ä—É–≥ –Ω–∞ –¥—Ä—É–≥–∞. –í—ã—Å–æ–∫–∞—è –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å ‚Äî –ø–ª–æ—Ö–æ (–º—ã —Ö–æ—Ç–∏–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç).\n",
    "\n",
    "### 2. Generation Phase (–ì–µ–Ω–µ—Ä–∞—Ü–∏—è)\n",
    "–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º **LLM-as-a-Judge** (–æ–¥–Ω–∞ LLM –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –¥—Ä—É–≥—É—é).\n",
    "\n",
    "*   **`Faithfulness` (Answer vs Context):** –í–µ—Ä–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç—É. –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –æ—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω **–∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ** –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –±–µ–∑ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π.\n",
    "*   **`Answer Relevancy` (Answer vs Query):** –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –≤–æ–ø—Ä–æ—Å–∞. –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8TaLFbIhR-st",
   "metadata": {
    "id": "8TaLFbIhR-st"
   },
   "outputs": [],
   "source": [
    "import ir_measures\n",
    "from ir_measures import nDCG, Recall, RR\n",
    "\n",
    "RUNS_CSV_PATH = os.path.join(LOGS_DIR, \"runs.csv\")\n",
    "\n",
    "def compute_redundancy(contexts: List[str]) -> float:\n",
    "    \"\"\"Calculates semantic redundancy (higher = more duplicate info).\"\"\"\n",
    "    if not contexts: return 0.0\n",
    "    unique = len(set(contexts))\n",
    "    return 1.0 - (unique / len(contexts))\n",
    "\n",
    "def compute_retrieval_metrics(qrels_map, run_doc_ids, run_contexts, run_scores, k=5):\n",
    "    # 1. Reference-based Metrics (Ground Truth needed)\n",
    "    run_ir = {}\n",
    "    for qid, doc_ids in run_doc_ids.items():\n",
    "        run_ir[qid] = {doc_id: float(-(rank + 1)) for rank, doc_id in enumerate(doc_ids[:k])}\n",
    "\n",
    "    measures = [nDCG@k, Recall@k]\n",
    "    agg = ir_measures.calc_aggregate(measures, qrels_map, run_ir)\n",
    "\n",
    "    # 2. Reference-free Metrics (No Ground Truth)\n",
    "    red_vals = [compute_redundancy(ctxs[:k]) for ctxs in run_contexts.values()]\n",
    "    # Mean Score: How confident is the retriever?\n",
    "    score_vals = [np.mean(scores[:k]) for scores in run_scores.values() if scores]\n",
    "\n",
    "    return {\n",
    "        \"ndcg@k\": float(agg[nDCG@k]),\n",
    "        \"recall@k\": float(agg[Recall@k]),\n",
    "        \"ref_free_mean_score\": float(np.mean(score_vals)) if score_vals else 0.0,\n",
    "        \"ref_free_redundancy\": float(np.mean(red_vals)) if red_vals else 0.0\n",
    "    }\n",
    "\n",
    "def log_run(cfg, split, stage, run_name, metrics):\n",
    "    row = {\n",
    "        \"ts\": time.strftime(\"%H:%M:%S\", time.gmtime()),\n",
    "        \"run_name\": run_name,\n",
    "        \"split\": split,\n",
    "        \"stage\": stage,\n",
    "        **metrics\n",
    "    }\n",
    "    df_old = pd.read_csv(RUNS_CSV_PATH) if os.path.exists(RUNS_CSV_PATH) else pd.DataFrame()\n",
    "    pd.concat([df_old, pd.DataFrame([row])], ignore_index=True).to_csv(RUNS_CSV_PATH, index=False)\n",
    "\n",
    "def show_leaderboard(stage=\"retrieval\", split=\"main\"):\n",
    "    if not os.path.exists(RUNS_CSV_PATH): return\n",
    "    df = pd.read_csv(RUNS_CSV_PATH)\n",
    "    df_sub = df[(df[\"stage\"]==stage) & (df[\"split\"]==split)].copy()\n",
    "    if df_sub.empty: return\n",
    "\n",
    "    # Sort by key metric\n",
    "    sort_col = \"ndcg@k\" if stage == \"retrieval\" else \"faithfulness\"\n",
    "    if sort_col in df_sub.columns:\n",
    "        df_sub = df_sub.sort_values(sort_col, ascending=False)\n",
    "\n",
    "    # Select columns to display\n",
    "    cols_ret = [\"run_name\", \"ndcg@k\", \"recall@k\", \"ref_free_mean_score\", \"ref_free_redundancy\", \"latency_s\"]\n",
    "    cols_gen = [\"run_name\", \"faithfulness\", \"answer_relevancy\", \"latency_s\"]\n",
    "\n",
    "    cols = cols_ret if stage == \"retrieval\" else cols_gen\n",
    "    print(f\"\\nüèÜ LEADERBOARD [{stage.upper()} | {split.upper()}] üèÜ\")\n",
    "    display(df_sub[[c for c in cols if c in df_sub.columns]])\n",
    "\n",
    "def _get_df(split): return df_queries[df_queries[\"query_id\"].isin(split_main if split==\"main\" else split_challenge)]\n",
    "\n",
    "def run_retrieval(cfg: RAGConfig, split: str, run_name: str):\n",
    "    unload_embedder()\n",
    "    index, retriever, _ = build_rag_pipeline(cfg, df_corpus, load_llm=False)\n",
    "\n",
    "    df_q = _get_df(split)\n",
    "    results_dids, results_txts, results_scores = {}, {}, {}\n",
    "\n",
    "    t0 = time.time()\n",
    "    for row in tqdm(df_q.itertuples(index=False), total=len(df_q), desc=f\"Retr {run_name}\"):\n",
    "        nodes = retriever.retrieve(row.text)\n",
    "        # De-duplicate by doc_id to avoid metrics skew\n",
    "        seen, dids, txts, scores = set(), [], [], []\n",
    "        for n in nodes:\n",
    "            if n.metadata[\"doc_id\"] not in seen:\n",
    "                seen.add(n.metadata[\"doc_id\"])\n",
    "                dids.append(n.metadata[\"doc_id\"])\n",
    "                txts.append(n.get_text())\n",
    "                scores.append(n.score if n.score else 0.0)\n",
    "        results_dids[str(row.query_id)] = dids\n",
    "        results_txts[str(row.query_id)] = txts\n",
    "        results_scores[str(row.query_id)] = scores\n",
    "\n",
    "    # FIX: filter qrels_map based on current split\n",
    "    split_qids = set(df_q[\"query_id\"].astype(str))\n",
    "    qrels_map_filtered = {qid: docs for qid, docs in qrels_map.items() if qid in split_qids}\n",
    "    \n",
    "    latency = time.time() - t0\n",
    "\n",
    "    metrics = compute_retrieval_metrics(qrels_map_filtered, results_dids, results_txts, results_scores, k=cfg.retrieval.top_k)\n",
    "    metrics[\"latency_s\"] = latency\n",
    "    log_run(cfg, split, \"retrieval\", run_name, metrics)\n",
    "    show_leaderboard(\"retrieval\", split)\n",
    "    unload_embedder()\n",
    "\n",
    "def run_generation(cfg: RAGConfig, split: str, run_name: str):\n",
    "    \"\"\"Generates answers and saves them to JSON for subsequent evaluation.\"\"\"\n",
    "    # 1. Retrieval Phase\n",
    "    unload_embedder()\n",
    "    index, retriever, _ = build_rag_pipeline(cfg, df_corpus, load_llm=False)\n",
    "\n",
    "    df_q = _get_df(split).head(cfg.llm.e2e_eval_n)\n",
    "    records = []\n",
    "    for row in tqdm(df_q.itertuples(index=False), total=len(df_q), desc=\"Retrieving Contexts\"):\n",
    "        nodes = retriever.retrieve(row.text)\n",
    "        ctx_list = [n.get_text() for n in nodes[:cfg.retrieval.top_k]]\n",
    "        records.append({\"query_id\": str(row.query_id), \"q\": row.text, \"ctx\": \"\\n\".join(ctx_list)})\n",
    "\n",
    "    unload_embedder()\n",
    "\n",
    "    # 2. Generation Phase\n",
    "    Settings.embed_model = None\n",
    "    llm = get_llm(cfg.llm)\n",
    "\n",
    "    t0 = time.time()\n",
    "    for item in tqdm(records, desc=\"Generating\"):\n",
    "        prompt = cfg.llm.prompt_template.format(context_str=item[\"ctx\"], query_str=item[\"q\"])\n",
    "        try: \n",
    "            item[\"a\"] = llm.complete(prompt).text\n",
    "        except: \n",
    "            item[\"a\"] = \"\"\n",
    "    latency = time.time() - t0\n",
    "\n",
    "    # 3. Save results\n",
    "    output_path = os.path.join(LOGS_DIR, f\"generations_{run_name}_{split}.json\")\n",
    "    import json\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump({\n",
    "            \"records\": records, \n",
    "            \"latency_s\": latency, \n",
    "            \"cfg_name\": cfg.name,\n",
    "            \"run_name\": run_name,\n",
    "            \"split\": split\n",
    "        }, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Generations saved to {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "\n",
    "def run_evaluation(generation_file: str, eval_name: Optional[str] = None):\n",
    "    \"\"\"Evaluates already generated answers.\"\"\"\n",
    "    import json\n",
    "    \n",
    "    # Load generated data\n",
    "    with open(generation_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    records = data[\"records\"]\n",
    "    latency = data[\"latency_s\"]\n",
    "    split = data.get(\"split\", \"main\")\n",
    "    run_name = eval_name if eval_name else data.get(\"run_name\", \"eval\")\n",
    "    \n",
    "    # Load LLM for Judge\n",
    "    Settings.embed_model = None\n",
    "    llm = get_llm(LLMConfig())\n",
    "    \n",
    "    # Judge Phase\n",
    "    faith_scores = []\n",
    "    rel_scores = []\n",
    "\n",
    "    for item in tqdm(records, desc=f\"Judging {run_name}\"):\n",
    "        if not item.get(\"a\", \"\"):\n",
    "            faith_scores.append(0.0); rel_scores.append(0.0)\n",
    "            continue\n",
    "\n",
    "        # Metric 1: Faithfulness (Context vs Answer)\n",
    "        p_faith = f\"Context: {item['ctx'][:1000]}\\nAnswer: {item['a']}\\nDoes the Answer use the Context? YES/NO.\"\n",
    "        res_f = llm.complete(p_faith, max_new_tokens=5).text.upper()\n",
    "        faith_scores.append(1.0 if \"YES\" in res_f else 0.0)\n",
    "\n",
    "        # Metric 2: Answer Relevancy (Question vs Answer)\n",
    "        p_rel = f\"Question: {item['q']}\\nAnswer: {item['a']}\\nDoes the Answer address the Question? YES/NO.\"\n",
    "        res_r = llm.complete(p_rel, max_new_tokens=5).text.upper()\n",
    "        rel_scores.append(1.0 if \"YES\" in res_r else 0.0)\n",
    "\n",
    "    metrics = {\n",
    "        \"faithfulness\": np.mean(faith_scores),\n",
    "        \"answer_relevancy\": np.mean(rel_scores),\n",
    "        \"latency_s\": latency\n",
    "    }\n",
    "    \n",
    "    log_run(RAGConfig(name=run_name), split, \"e2e\", run_name, metrics)\n",
    "    show_leaderboard(\"e2e\", split)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def run_e2e(cfg: RAGConfig, split: str, run_name: str, generation_file: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    End-to-end evaluation with option to reuse generations.\n",
    "    \n",
    "    Args:\n",
    "        cfg: RAG configuration\n",
    "        split: \"main\" or \"challenge\"\n",
    "        run_name: Experiment name\n",
    "        generation_file: Path to previously generated answers file (optional)\n",
    "    \"\"\"\n",
    "    if generation_file and os.path.exists(generation_file):\n",
    "        # Reuse existing generation\n",
    "        print(f\"üìÇ Loading generations from {generation_file}\")\n",
    "        return run_evaluation(generation_file, run_name)\n",
    "    else:\n",
    "        # Full cycle: generation + evaluation\n",
    "        gen_file = run_generation(cfg, split, run_name)\n",
    "        return run_evaluation(gen_file, run_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "J74iFYulR-st",
   "metadata": {
    "id": "J74iFYulR-st"
   },
   "source": [
    "## Task 0: –†–µ–∞–ª–∏–∑–∞—Ü–∏—è Baseline Pipeline\n",
    "\n",
    "–í–∞—à–∞ –ø–µ—Ä–≤–∞—è –∑–∞–¥–∞—á–∞ ‚Äî —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é `build_rag_pipeline`. –ù–∞ –¥–∞–Ω–Ω–æ–º —ç—Ç–∞–ø–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è —Å–æ–±—Ä–∞—Ç—å –ø—Ä–æ—Å—Ç—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Dense Retrieval.\n",
    "\n",
    "**–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏:**\n",
    "1.  –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ `Settings` (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ `get_embedder` –∏ `get_llm`).\n",
    "2.  –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ `QdrantVectorStore`.\n",
    "3.  –õ–æ–≥–∏–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: –ï—Å–ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏—è –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç (–∏–ª–∏ `recreate_collection=True`), —Å–æ–∑–¥–∞—Ç—å –µ—ë –∏–∑ `df_corpus`, –∏—Å–ø–æ–ª—å–∑—É—è `SentenceSplitter`. –ï—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç ‚Äî –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å.\n",
    "4.  –í–æ–∑–≤—Ä–∞—Ç –æ–±—ä–µ–∫—Ç–æ–≤ `index`, `retriever` –∏ `query_engine`.\n",
    "\n",
    "–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä `overfetch_k` –≤ –∫–æ–Ω—Ñ–∏–≥–µ. –†–µ—Ç—Ä–∏–≤–µ—Ä –¥–æ–ª–∂–µ–Ω –∏–∑–≤–ª–µ–∫–∞—Ç—å –∏–º–µ–Ω–Ω–æ —ç—Ç–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8585b8f-67aa-40c5-8c46-329826d297a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Tuple\n",
    "from dataclasses import replace\n",
    "\n",
    "from llama_index.core.schema import NodeWithScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a9e299d-370e-4ae0-8e94-aa4079bde1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper\n",
    "def _collection_exists(client: QdrantClient, name: str) -> bool:\n",
    "    try:\n",
    "        return bool(client.collection_exists(name))  # newer qdrant-client\n",
    "    except Exception:\n",
    "        try:\n",
    "            cols = client.get_collections()\n",
    "            return any(c.name == name for c in cols.collections)\n",
    "        except Exception:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fdcb699-c06d-4ec8-a58b-499da02bc05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid RRF retriever (robust, no dependency on QueryFusionRetriever API)\n",
    "class HybridRRFRetriever:\n",
    "    \"\"\"\n",
    "    Simple Reciprocal Rank Fusion over two retrievers.\n",
    "    Returns top_k NodeWithScore with fused score.\n",
    "    \"\"\"\n",
    "    def __init__(self, dense_retriever, bm25_retriever, top_k: int = 20, rrf_k: int = 60):\n",
    "        self.dense = dense_retriever\n",
    "        self.bm25 = bm25_retriever\n",
    "        self.top_k = top_k\n",
    "        self.rrf_k = rrf_k\n",
    "\n",
    "    def retrieve(self, query: str):\n",
    "        dense_nodes = self.dense.retrieve(query) or []\n",
    "        bm25_nodes = self.bm25.retrieve(query) or []\n",
    "\n",
    "        # Key for fusion: prefer (doc_id + text hash) to avoid collisions\n",
    "        def _key(n: Any) -> str:\n",
    "            md = getattr(n, \"metadata\", {}) or {}\n",
    "            doc_id = str(md.get(\"doc_id\", \"\"))\n",
    "            txt = \"\"\n",
    "            try:\n",
    "                txt = n.get_text()\n",
    "            except Exception:\n",
    "                try:\n",
    "                    txt = n.node.get_text()\n",
    "                except Exception:\n",
    "                    txt = \"\"\n",
    "            return f\"{doc_id}::{hash(txt)}\"\n",
    "\n",
    "        fused = {}  # key -> (best_node, score)\n",
    "\n",
    "        def _add(list_nodes, weight=1.0):\n",
    "            for rank, n in enumerate(list_nodes):\n",
    "                k = _key(n)\n",
    "                score = weight * (1.0 / (self.rrf_k + rank + 1))\n",
    "                if k not in fused:\n",
    "                    fused[k] = [n, 0.0]\n",
    "                fused[k][1] += score\n",
    "\n",
    "        _add(dense_nodes)\n",
    "        _add(bm25_nodes)\n",
    "\n",
    "        out = []\n",
    "        for n, s in fused.values():\n",
    "            # set/update score if possible\n",
    "            try:\n",
    "                n.score = float(s)\n",
    "                out.append(n)\n",
    "            except Exception:\n",
    "                # ensure NodeWithScore shape if needed\n",
    "                try:\n",
    "                    out.append(NodeWithScore(node=n.node, score=float(s)))\n",
    "                except Exception:\n",
    "                    out.append(n)\n",
    "\n",
    "        out.sort(key=lambda x: float(getattr(x, \"score\", 0.0) or 0.0), reverse=True)\n",
    "        return out[: self.top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09e08bb9-5efd-4d2c-a53d-8730f07a45e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_CACHED_RERANK = None\n",
    "\n",
    "def unload_reranker():\n",
    "    global _CACHED_RERANK\n",
    "    if _CACHED_RERANK is not None:\n",
    "        tok, mdl, *_ = _CACHED_RERANK\n",
    "        del tok, mdl\n",
    "        _CACHED_RERANK = None\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "def _format_instruction(instruction: str | None, query: str, doc: str) -> str:\n",
    "    if instruction is None:\n",
    "        # –º–æ–∂–Ω–æ —á—É—Ç—å \"–∑–∞—Ç–æ—á–∏—Ç—å\" –ø–æ–¥ SciFact\n",
    "        instruction = \"Given a scientific claim, retrieve passages that support or refute it.\"\n",
    "    return f\"<Instruct>: {instruction}\\n<Query>: {query}\\n<Document>: {doc}\"\n",
    "\n",
    "def get_reranker(model_name: str, max_reranker_length: int = 2048):\n",
    "    \"\"\"\n",
    "    Qwen3 reranker is a generative reranker:\n",
    "    score = P(yes | formatted_input).\n",
    "    \"\"\"\n",
    "    global _CACHED_RERANK\n",
    "    if _CACHED_RERANK is not None:\n",
    "        return _CACHED_RERANK\n",
    "    \n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "    bnb = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "    )\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, padding_side=\"left\")\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "\n",
    "    mdl = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "    ).eval()\n",
    "\n",
    "    token_false_id = tok.convert_tokens_to_ids(\"no\")\n",
    "    token_true_id  = tok.convert_tokens_to_ids(\"yes\")\n",
    "\n",
    "    # —ç—Ç–æ—Ç —à–∞–±–ª–æ–Ω (prefix/suffix) ‚Äî –∏–∑ –ø—Ä–∏–º–µ—Ä–æ–≤ Qwen3 reranker\n",
    "    prefix = (\n",
    "        '<|im_start|>system\\n'\n",
    "        ' Judge whether the Document meets the requirements based on the Query and the Instruct provided. '\n",
    "        'Note that the answer can only be \"yes\" or \"no\".<|im_end|>\\n'\n",
    "        '<|im_start|>user\\n'\n",
    "    )\n",
    "    suffix = '<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n'\n",
    "\n",
    "    prefix_tokens = tok.encode(prefix, add_special_tokens=False)\n",
    "    suffix_tokens = tok.encode(suffix, add_special_tokens=False)\n",
    "\n",
    "    _CACHED_RERANK = (tok, mdl, token_true_id, token_false_id, prefix_tokens, suffix_tokens, max_reranker_length)\n",
    "    return _CACHED_RERANK\n",
    "\n",
    "@torch.no_grad()\n",
    "def qwen_rerank_scores(query: str, docs: list[str], model_name: str, batch_size: int = 8, instruction: str | None = None):\n",
    "    import torch.nn.functional as F\n",
    "    \n",
    "    tok, mdl, true_id, false_id, prefix_toks, suffix_toks, max_len = get_reranker(model_name)\n",
    "\n",
    "    pairs = [_format_instruction(instruction, query, d) for d in docs]\n",
    "\n",
    "    scores = []\n",
    "    for i in range(0, len(pairs), batch_size):\n",
    "        batch_pairs = pairs[i:i+batch_size]\n",
    "\n",
    "        inputs = tok(\n",
    "            batch_pairs,\n",
    "            padding=False,\n",
    "            truncation=\"longest_first\",\n",
    "            return_attention_mask=False,\n",
    "            max_length=max_len - len(prefix_toks) - len(suffix_toks),\n",
    "        )\n",
    "\n",
    "        # –¥–æ–±–∞–≤–ª—è–µ–º prefix/suffix\n",
    "        for j, ids in enumerate(inputs[\"input_ids\"]):\n",
    "            inputs[\"input_ids\"][j] = prefix_toks + ids + suffix_toks\n",
    "\n",
    "        inputs = tok.pad(inputs, padding=\"max_length\", return_tensors=\"pt\", max_length=max_len)\n",
    "\n",
    "        # –ø–µ—Ä–µ–Ω–æ—Å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏\n",
    "        inputs = {k: v.to(mdl.device) for k, v in inputs.items()}\n",
    "\n",
    "        logits = mdl(**inputs).logits[:, -1, :]          # (B, vocab)\n",
    "        yes = logits[:, true_id]\n",
    "        no  = logits[:, false_id]\n",
    "        two = torch.stack([no, yes], dim=1)              # (B,2)\n",
    "        p_yes = F.log_softmax(two, dim=1)[:, 1].exp()    # –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å \"yes\"\n",
    "        scores.extend(p_yes.detach().float().cpu().tolist())\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97c30e61-3a23-4ec9-832b-dd01c6ae064a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RerankRetriever:\n",
    "    def __init__(self, base_retriever, model_name: str, top_n: int, batch_size: int = 8):\n",
    "        self.base = base_retriever\n",
    "        self.model_name = model_name\n",
    "        self.top_n = top_n\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def retrieve(self, query: str):\n",
    "        nodes = self.base.retrieve(query)\n",
    "        if not nodes:\n",
    "            return nodes\n",
    "\n",
    "        texts = [n.get_text() for n in nodes]\n",
    "        scores = qwen_rerank_scores(\n",
    "            query=query,\n",
    "            docs=texts,\n",
    "            model_name=self.model_name,\n",
    "            batch_size=self.batch_size,\n",
    "            instruction=\"Given a scientific claim, retrieve passages that support or refute it.\"\n",
    "        )\n",
    "\n",
    "        ranked = sorted(zip(nodes, scores), key=lambda x: x[1], reverse=True)\n",
    "        out = []\n",
    "        for n, s in ranked[: self.top_n]:\n",
    "            n.score = float(s)\n",
    "            out.append(n)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aceb2ab1-ecb5-4f68-93dc-af7f1cfe0137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.http import models as qmodels\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "_QDRANT_CLIENT = None\n",
    "\n",
    "def get_qdrant_client():\n",
    "    global _QDRANT_CLIENT\n",
    "    if _QDRANT_CLIENT is None:\n",
    "        _QDRANT_CLIENT = QdrantClient(path=QDRANT_PATH)\n",
    "    return _QDRANT_CLIENT\n",
    "\n",
    "def close_qdrant_client():\n",
    "    global _QDRANT_CLIENT\n",
    "    if _QDRANT_CLIENT is not None:\n",
    "        try:\n",
    "            _QDRANT_CLIENT.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "        _QDRANT_CLIENT = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "WvbMet4ZR-st",
   "metadata": {
    "id": "WvbMet4ZR-st"
   },
   "outputs": [],
   "source": [
    "def build_rag_pipeline(cfg: RAGConfig, df_corpus: pd.DataFrame, load_llm: bool = True):\n",
    "    # 0. Ensure paths exist\n",
    "    os.makedirs(QDRANT_PATH, exist_ok=True)\n",
    "    # 1. Setup Models\n",
    "    Settings.embed_model = get_embedder(cfg.embedding)\n",
    "    Settings.llm = get_llm(cfg.llm) if load_llm else None\n",
    "    \n",
    "    # 2. Vector Store Setup \n",
    "    client = get_qdrant_client()\n",
    "\n",
    "    dim = len(Settings.embed_model.get_text_embedding(\"dim_probe\"))\n",
    "\n",
    "    # recreate / create collection explicitly\n",
    "    if cfg.recreate_collection:\n",
    "        client.recreate_collection(\n",
    "            collection_name=cfg.qdrant_collection,\n",
    "            vectors_config=qmodels.VectorParams(size=dim, distance=qmodels.Distance.COSINE),\n",
    "        )\n",
    "        must_reindex = True\n",
    "    else:\n",
    "        # –∏–Ω–∞—á–µ –ø—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å; –µ—Å–ª–∏ –Ω–µ—Ç ‚Äî —Å–æ–∑–¥–∞—ë–º\n",
    "        try:\n",
    "            client.get_collection(cfg.qdrant_collection)\n",
    "            must_reindex = False\n",
    "        except Exception:\n",
    "            client.create_collection(\n",
    "                collection_name=cfg.qdrant_collection,\n",
    "                vectors_config=qmodels.VectorParams(size=dim, distance=qmodels.Distance.COSINE),\n",
    "            )\n",
    "            must_reindex = True\n",
    "\n",
    "    vector_store = QdrantVectorStore(client=client, collection_name=cfg.qdrant_collection)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "    # 3. Indexing Logic \n",
    "    # Check if collection exists.\n",
    "    # If yes & not recreate -> load index.\n",
    "    # Else -> create documents, setup splitter, build index.\n",
    "    if (not must_reindex) and (not cfg.chunking.use_hierarchical):\n",
    "        # –±—ã—Å—Ç—Ä—ã–π –ø—É—Ç—å: –∫–æ–ª–ª–µ–∫—Ü–∏—è —É–∂–µ –µ—Å—Ç—å, –ø—Ä–æ—Å—Ç–æ –ø–æ–¥–∫–ª—é—á–∞–µ–º –∏–Ω–¥–µ–∫—Å –∫ vector_store\n",
    "        index = VectorStoreIndex.from_vector_store(vector_store=vector_store, storage_context=storage_context)\n",
    "    else:\n",
    "        # —Å–æ–∑–¥–∞—ë–º –¥–æ–∫—É–º–µ–Ω—Ç—ã\n",
    "        documents = []\n",
    "        for r in df_corpus.itertuples(index=False):\n",
    "            full_text = (str(r.title).strip() + \"\\n\\n\" + str(r.text).strip()).strip()\n",
    "            documents.append(\n",
    "                Document(\n",
    "                    text=full_text,\n",
    "                    metadata={\"doc_id\": str(r.doc_id), \"title\": str(getattr(r, \"title\", \"\") or \"\")},\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if cfg.chunking.use_hierarchical:\n",
    "            # Hierarchical: –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º leaf, –∞ –ø–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º parent —á–µ—Ä–µ–∑ AutoMergingRetriever\n",
    "            parser = HierarchicalNodeParser.from_defaults(\n",
    "                chunk_sizes=[cfg.chunking.chunk_size, cfg.chunking.child_chunk_size],\n",
    "                chunk_overlap=cfg.chunking.chunk_overlap,\n",
    "            )\n",
    "            all_nodes = parser.get_nodes_from_documents(documents)\n",
    "\n",
    "            leaf_nodes = []\n",
    "            for n in all_nodes:\n",
    "                child = getattr(n, \"child_nodes\", None)\n",
    "                if not child:\n",
    "                    leaf_nodes.append(n)\n",
    "\n",
    "            # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –Ω–æ–¥—ã –≤ docstore, —á—Ç–æ–±—ã AutoMerging —Å–º–æ–≥ —Å–∫–ª–µ–∏–≤–∞—Ç—å\n",
    "            try:\n",
    "                storage_context.docstore.add_documents(all_nodes)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            index = VectorStoreIndex(nodes=leaf_nodes, storage_context=storage_context, show_progress=True)\n",
    "        else:\n",
    "            splitter = SentenceSplitter(\n",
    "                chunk_size=cfg.chunking.chunk_size,\n",
    "                chunk_overlap=cfg.chunking.chunk_overlap,\n",
    "            )\n",
    "            index = VectorStoreIndex.from_documents(\n",
    "                documents,\n",
    "                storage_context=storage_context,\n",
    "                transformations=[splitter],\n",
    "                show_progress=True,\n",
    "            )\n",
    "\n",
    "    # 4. Retrieval Construction (TODO)\n",
    "    # retriever = ... (use cfg.retrieval.overfetch_k)\n",
    "    # query_engine = ...\n",
    "    dense_retriever = index.as_retriever(similarity_top_k=cfg.retrieval.overfetch_k)\n",
    "\n",
    "    # Optional: hierarchical ‚Äúreturn parents‚Äù\n",
    "    retriever = dense_retriever\n",
    "    if cfg.chunking.use_hierarchical:\n",
    "        try:\n",
    "            from llama_index.core.retrievers import AutoMergingRetriever\n",
    "            retriever = AutoMergingRetriever(dense_retriever, storage_context, verbose=False)\n",
    "        except Exception:\n",
    "            # fallback: just dense leaf chunks\n",
    "            retriever = dense_retriever\n",
    "\n",
    "    # Hybrid mode (dense + BM25) with RRF\n",
    "    if cfg.retrieval.mode == \"hybrid\":\n",
    "        # Build BM25 over the SAME chunking strategy (best-effort).\n",
    "        # NOTE: BM25 doesn't depend on qdrant; we can create it on the fly.\n",
    "        if not cfg.chunking.use_hierarchical:\n",
    "            splitter = SentenceSplitter(\n",
    "                chunk_size=cfg.chunking.chunk_size,\n",
    "                chunk_overlap=cfg.chunking.chunk_overlap,\n",
    "            )\n",
    "            docs_for_bm25 = []\n",
    "            for r in df_corpus.itertuples(index=False):\n",
    "                full_text = (str(r.title).strip() + \"\\n\\n\" + str(r.text).strip()).strip()\n",
    "                docs_for_bm25.append(\n",
    "                    Document(text=full_text, metadata={\"doc_id\": str(r.doc_id), \"title\": str(r.title or \"\")})\n",
    "                )\n",
    "            bm25_nodes = splitter.get_nodes_from_documents(docs_for_bm25)\n",
    "        else:\n",
    "            # For hierarchical: use leaf nodes as bm25 units\n",
    "            # (parents are reconstructed by AutoMerging on dense path; bm25 stays leaf-level)\n",
    "            try:\n",
    "                # if we indexed from scratch, docstore may contain leaf nodes; otherwise rebuild quickly\n",
    "                bm25_nodes = []\n",
    "                # safest rebuild:\n",
    "                parser = HierarchicalNodeParser.from_defaults(\n",
    "                    chunk_sizes=[cfg.chunking.chunk_size, cfg.chunking.child_chunk_size],\n",
    "                    chunk_overlap=cfg.chunking.chunk_overlap,\n",
    "                )\n",
    "                docs_for_bm25 = []\n",
    "                for r in df_corpus.itertuples(index=False):\n",
    "                    full_text = (str(r.title).strip() + \"\\n\\n\" + str(r.text).strip()).strip()\n",
    "                    docs_for_bm25.append(Document(text=full_text, metadata={\"doc_id\": str(r.doc_id), \"title\": str(r.title or \"\")}))\n",
    "                all_nodes = parser.get_nodes_from_documents(docs_for_bm25)\n",
    "                for n in all_nodes:\n",
    "                    child = getattr(n, \"child_nodes\", None)\n",
    "                    if not child:\n",
    "                        bm25_nodes.append(n)\n",
    "            except Exception:\n",
    "                bm25_nodes = []\n",
    "\n",
    "        bm25_retriever = BM25Retriever.from_defaults(\n",
    "            nodes=bm25_nodes,\n",
    "            similarity_top_k=cfg.retrieval.overfetch_k,\n",
    "        )\n",
    "\n",
    "        retriever = HybridRRFRetriever(\n",
    "            dense_retriever=dense_retriever,\n",
    "            bm25_retriever=bm25_retriever,\n",
    "            top_k=cfg.retrieval.overfetch_k,\n",
    "            rrf_k=60,\n",
    "        )\n",
    "\n",
    "    # Optional reranking (funnel: overfetch_k -> rerank_top_n)\n",
    "    if cfg.retrieval.use_reranker:\n",
    "        retriever = RerankRetriever(\n",
    "            base_retriever=retriever,\n",
    "            model_name=cfg.rerank_model,\n",
    "            top_n=max(cfg.retrieval.rerank_top_n, cfg.retrieval.top_k),\n",
    "            batch_size=8\n",
    "        )\n",
    "\n",
    "    # Query engine (not strictly needed for —Ç–≤–æ–∏—Ö run_* —Ñ—É–Ω–∫—Ü–∏–π, –Ω–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ —Ç—Ä–µ–±—É–µ—Ç—Å—è)\n",
    "    query_engine = index.as_query_engine(retriever=retriever) if load_llm else None\n",
    "\n",
    "    # return index, retriever, query_engine\n",
    "    return index, retriever, query_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "O7lqryDrR-st",
   "metadata": {
    "id": "O7lqryDrR-st",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Baseline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Soft\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1326: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  return t.to(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grigorii\\AppData\\Local\\Temp\\ipykernel_34096\\608474731.py:15: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e6a6582bfe54c47a808e430c31c4041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/5183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c4f07f0e864367907458b42a341775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e96dcfa090a42f19b80eacdfbc5e153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf07f92d93c94df6bafea650c5822a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/1650 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "523fad8ab2b04b7c8ba9ab555718155c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retr baseline:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ LEADERBOARD [RETRIEVAL | MAIN] üèÜ\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_name</th>\n",
       "      <th>ndcg@k</th>\n",
       "      <th>recall@k</th>\n",
       "      <th>ref_free_mean_score</th>\n",
       "      <th>ref_free_redundancy</th>\n",
       "      <th>latency_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline</td>\n",
       "      <td>0.674031</td>\n",
       "      <td>0.7695</td>\n",
       "      <td>0.573575</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.45268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   run_name    ndcg@k  recall@k  ref_free_mean_score  ref_free_redundancy  \\\n",
       "0  baseline  0.674031    0.7695             0.573575                  0.0   \n",
       "\n",
       "   latency_s  \n",
       "0   22.45268  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings have been explicitly disabled. Using MockEmbedding.\n",
      "\n",
      "üèÜ LEADERBOARD [RETRIEVAL | MAIN] üèÜ\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_name</th>\n",
       "      <th>ndcg@k</th>\n",
       "      <th>recall@k</th>\n",
       "      <th>ref_free_mean_score</th>\n",
       "      <th>ref_free_redundancy</th>\n",
       "      <th>latency_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline</td>\n",
       "      <td>0.674031</td>\n",
       "      <td>0.7695</td>\n",
       "      <td>0.573575</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.45268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   run_name    ndcg@k  recall@k  ref_free_mean_score  ref_free_redundancy  \\\n",
       "0  baseline  0.674031    0.7695             0.573575                  0.0   \n",
       "\n",
       "   latency_s  \n",
       "0   22.45268  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TEST YOUR BASELINE\n",
    "baseline_cfg = RAGConfig(\n",
    "    name=\"baseline\",\n",
    "    chunking=ChunkingConfig(chunk_size=512, chunk_overlap=50),\n",
    "    retrieval=RetrievalConfig(top_k=5, overfetch_k=15),\n",
    "    qdrant_collection=\"scifact_base_512\",\n",
    "    recreate_collection=True\n",
    ")\n",
    "\n",
    "print(\"Running Baseline...\")\n",
    "# 1. Run Retrieval\n",
    "# TODO: Uncomment after implementing pipeline\n",
    "run_retrieval(baseline_cfg, \"main\", \"baseline\")\n",
    "\n",
    "# 2. Check Results\n",
    "show_leaderboard(\"retrieval\", \"main\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MnmknYiKR-st",
   "metadata": {
    "id": "MnmknYiKR-st"
   },
   "source": [
    "## Task 1: Chunking Strategies\n",
    "\n",
    "–†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–ª–∏—è–µ—Ç –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞.\n",
    "\n",
    "**–ó–∞–¥–∞–Ω–∏–µ:**\n",
    "1.  **Small vs Large:** –°–æ–∑–¥–∞–π—Ç–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è `chunk_size=256` –∏ `chunk_size=1024`. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã. *–ù–∞–ø–æ–º–∏–Ω–∞–Ω–∏–µ: –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ —á–∞–Ω–∫–∏–Ω–≥–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –º–µ–Ω—è—Ç—å –∏–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –∏–ª–∏ —Å—Ç–∞–≤–∏—Ç—å `recreate_collection=True`.*\n",
    "2.  **Hierarchical Chunking:** –†–µ–∞–ª–∏–∑—É–π—Ç–µ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π —á–∞–Ω–∫–∏–Ω–≥ (`use_hierarchical=True`). –î–ª—è —ç—Ç–æ–≥–æ –≤ `build_rag_pipeline` –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–æ–±–∞–≤–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É –∫–æ–Ω—Ñ–∏–≥–∞ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ `HierarchicalNodeParser`. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –º–µ–ª–∫–∏–µ —á–∞–Ω–∫–∏, –Ω–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏—Ö (–∫—Ä—É–ø–Ω—ã—Ö) –±–ª–æ–∫–æ–≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "kDOGzD6oR-st",
   "metadata": {
    "id": "kDOGzD6oR-st",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Soft\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1326: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  return t.to(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grigorii\\AppData\\Local\\Temp\\ipykernel_40064\\3262185644.py:15: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91df4a963b6b4118ba446b432bf01982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/5183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07bdf0e9bfac4145aece9ff27ab89731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e9627c1308457ca0c3ec6b1311147d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d1365b9d39478e921f38e06fb11f57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d03ad498804bb996fda017af602868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db041912531e4816bb9ec731c7655ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d8a0f13d2c345b7aff78641b6aa76db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/687 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aaf76939768450aad67213079c777f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retr chunk_256:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ LEADERBOARD [RETRIEVAL | MAIN] üèÜ\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_name</th>\n",
       "      <th>ndcg@k</th>\n",
       "      <th>recall@k</th>\n",
       "      <th>ref_free_mean_score</th>\n",
       "      <th>ref_free_redundancy</th>\n",
       "      <th>latency_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chunk_256</td>\n",
       "      <td>0.678425</td>\n",
       "      <td>0.76075</td>\n",
       "      <td>0.578076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.496207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline</td>\n",
       "      <td>0.674031</td>\n",
       "      <td>0.76950</td>\n",
       "      <td>0.573575</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.452680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    run_name    ndcg@k  recall@k  ref_free_mean_score  ref_free_redundancy  \\\n",
       "1  chunk_256  0.678425   0.76075             0.578076                  0.0   \n",
       "0   baseline  0.674031   0.76950             0.573575                  0.0   \n",
       "\n",
       "   latency_s  \n",
       "1  26.496207  \n",
       "0  22.452680  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings have been explicitly disabled. Using MockEmbedding.\n",
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grigorii\\AppData\\Local\\Temp\\ipykernel_40064\\3262185644.py:15: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea7fcc7f86b40b591899e2890412329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/5183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "470b291ba87649b788ee054f31fe9def",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f18e105cd84342944d8b44bfa2655b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b838f88723a46f6b2e82ad9f364075d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/1101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ac2134ef4834b56a5b41272dd83177d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retr chunk_1024:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ LEADERBOARD [RETRIEVAL | MAIN] üèÜ\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_name</th>\n",
       "      <th>ndcg@k</th>\n",
       "      <th>recall@k</th>\n",
       "      <th>ref_free_mean_score</th>\n",
       "      <th>ref_free_redundancy</th>\n",
       "      <th>latency_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chunk_256</td>\n",
       "      <td>0.678425</td>\n",
       "      <td>0.76075</td>\n",
       "      <td>0.578076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.496207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chunk_1024</td>\n",
       "      <td>0.674990</td>\n",
       "      <td>0.76325</td>\n",
       "      <td>0.572649</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.664925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline</td>\n",
       "      <td>0.674031</td>\n",
       "      <td>0.76950</td>\n",
       "      <td>0.573575</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.452680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     run_name    ndcg@k  recall@k  ref_free_mean_score  ref_free_redundancy  \\\n",
       "1   chunk_256  0.678425   0.76075             0.578076                  0.0   \n",
       "2  chunk_1024  0.674990   0.76325             0.572649                  0.0   \n",
       "0    baseline  0.674031   0.76950             0.573575                  0.0   \n",
       "\n",
       "   latency_s  \n",
       "1  26.496207  \n",
       "2  21.664925  \n",
       "0  22.452680  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings have been explicitly disabled. Using MockEmbedding.\n",
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grigorii\\AppData\\Local\\Temp\\ipykernel_40064\\3262185644.py:15: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb269237f444dcdb4708d0fc7798e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0efc6d1e674f4b3a887a29e95d646248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c19a809167714c32afe6b14268622599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a3a79b553e4e688ae48ef2ad40ffc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4633b2c094764b9b9815b9c5bd843144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e87a1166bbc74f75a6854f7610d2ee97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/1340 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7654f5ce7d9d43e08cfe5879a2dd37f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retr hierarchical:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ LEADERBOARD [RETRIEVAL | MAIN] üèÜ\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_name</th>\n",
       "      <th>ndcg@k</th>\n",
       "      <th>recall@k</th>\n",
       "      <th>ref_free_mean_score</th>\n",
       "      <th>ref_free_redundancy</th>\n",
       "      <th>latency_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hierarchical</td>\n",
       "      <td>0.683837</td>\n",
       "      <td>0.76700</td>\n",
       "      <td>0.565709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.681055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chunk_256</td>\n",
       "      <td>0.678425</td>\n",
       "      <td>0.76075</td>\n",
       "      <td>0.578076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.496207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chunk_1024</td>\n",
       "      <td>0.674990</td>\n",
       "      <td>0.76325</td>\n",
       "      <td>0.572649</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.664925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline</td>\n",
       "      <td>0.674031</td>\n",
       "      <td>0.76950</td>\n",
       "      <td>0.573575</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.452680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       run_name    ndcg@k  recall@k  ref_free_mean_score  ref_free_redundancy  \\\n",
       "3  hierarchical  0.683837   0.76700             0.565709                  0.0   \n",
       "1     chunk_256  0.678425   0.76075             0.578076                  0.0   \n",
       "2    chunk_1024  0.674990   0.76325             0.572649                  0.0   \n",
       "0      baseline  0.674031   0.76950             0.573575                  0.0   \n",
       "\n",
       "   latency_s  \n",
       "3  26.681055  \n",
       "1  26.496207  \n",
       "2  21.664925  \n",
       "0  22.452680  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings have been explicitly disabled. Using MockEmbedding.\n",
      "\n",
      "üèÜ LEADERBOARD [RETRIEVAL | MAIN] üèÜ\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_name</th>\n",
       "      <th>ndcg@k</th>\n",
       "      <th>recall@k</th>\n",
       "      <th>ref_free_mean_score</th>\n",
       "      <th>ref_free_redundancy</th>\n",
       "      <th>latency_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hierarchical</td>\n",
       "      <td>0.683837</td>\n",
       "      <td>0.76700</td>\n",
       "      <td>0.565709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.681055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chunk_256</td>\n",
       "      <td>0.678425</td>\n",
       "      <td>0.76075</td>\n",
       "      <td>0.578076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.496207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chunk_1024</td>\n",
       "      <td>0.674990</td>\n",
       "      <td>0.76325</td>\n",
       "      <td>0.572649</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.664925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline</td>\n",
       "      <td>0.674031</td>\n",
       "      <td>0.76950</td>\n",
       "      <td>0.573575</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.452680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       run_name    ndcg@k  recall@k  ref_free_mean_score  ref_free_redundancy  \\\n",
       "3  hierarchical  0.683837   0.76700             0.565709                  0.0   \n",
       "1     chunk_256  0.678425   0.76075             0.578076                  0.0   \n",
       "2    chunk_1024  0.674990   0.76325             0.572649                  0.0   \n",
       "0      baseline  0.674031   0.76950             0.573575                  0.0   \n",
       "\n",
       "   latency_s  \n",
       "3  26.681055  \n",
       "1  26.496207  \n",
       "2  21.664925  \n",
       "0  22.452680  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Update build_rag_pipeline to support HierarchicalNodeParser\n",
    "\n",
    "# TODO: Define Configs\n",
    "# cfg_256 = ...\n",
    "# cfg_1024 = ...\n",
    "# cfg_hier = ...\n",
    "cfg_256 = RAGConfig(\n",
    "    name=\"chunk_256\",\n",
    "    chunking=ChunkingConfig(chunk_size=256, chunk_overlap=50),\n",
    "    retrieval=RetrievalConfig(top_k=5, overfetch_k=20, mode=\"dense\"),\n",
    "    qdrant_collection=\"scifact_chunk_256\",\n",
    "    recreate_collection=True\n",
    ")\n",
    "\n",
    "cfg_1024 = RAGConfig(\n",
    "    name=\"chunk_1024\",\n",
    "    chunking=ChunkingConfig(chunk_size=1024, chunk_overlap=100),\n",
    "    retrieval=RetrievalConfig(top_k=5, overfetch_k=20, mode=\"dense\"),\n",
    "    qdrant_collection=\"scifact_chunk_1024\",\n",
    "    recreate_collection=True\n",
    ")\n",
    "\n",
    "cfg_hier = RAGConfig(\n",
    "    name=\"hierarchical\",\n",
    "    chunking=ChunkingConfig(\n",
    "        chunk_size=1024,         # parent\n",
    "        child_chunk_size=256,    # leaf indexed\n",
    "        chunk_overlap=80,\n",
    "        use_hierarchical=True\n",
    "    ),\n",
    "    retrieval=RetrievalConfig(top_k=5, overfetch_k=25, mode=\"dense\"),\n",
    "    qdrant_collection=\"scifact_hier_1024_256\",\n",
    "    recreate_collection=True,  # —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ (–Ω—É–∂–Ω—ã –æ—Ç–Ω–æ—à–µ–Ω–∏—è parent/child)\n",
    ")\n",
    "# TODO: Run Experiments\n",
    "# run_retrieval(cfg_256, \"main\", \"chunk_256\")\n",
    "# show_leaderboard(\"retrieval\", \"main\")\n",
    "run_retrieval(cfg_256, \"main\", \"chunk_256\")\n",
    "run_retrieval(cfg_1024, \"main\", \"chunk_1024\")\n",
    "run_retrieval(cfg_hier, \"main\", \"hierarchical\")\n",
    "\n",
    "show_leaderboard(\"retrieval\", \"main\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12gsBiYhR-st",
   "metadata": {
    "id": "12gsBiYhR-st"
   },
   "source": [
    "## Task 2: Optimization & Advanced Retrieval\n",
    "\n",
    "–í —ç—Ç–æ–º –±–ª–æ–∫–µ –º—ã —Ä–µ–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏.\n",
    "\n",
    "### 2.1 Matryoshka Embeddings (Optimization)\n",
    "–ú–æ–¥–µ–ª—å `Qwen/Qwen3-Embedding` –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç **Matryoshka Representation Learning (MRL)**. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —É—Å–µ–∫–∞—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å 1024 –¥–æ 512) —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –ø–æ—Ç–µ—Ä–µ–π –∫–∞—á–µ—Å—Ç–≤–∞, —á—Ç–æ —ç–∫–æ–Ω–æ–º–∏—Ç –ø–∞–º—è—Ç—å –∏ —É—Å–∫–æ—Ä—è–µ—Ç –ø–æ–∏—Å–∫.\n",
    "**–ó–∞–¥–∞–Ω–∏–µ:** –ò–∑–º–µ–Ω–∏—Ç–µ `truncate_dim` –≤ `EmbeddingConfig` (–Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–∞ 512) –∏ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –≤–ª–∏—è–Ω–∏–µ –Ω–∞ –º–µ—Ç—Ä–∏–∫–∏.\n",
    "\n",
    "### 2.2 Advanced Retrieval (Hybrid + Rerank)\n",
    "–í–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å `build_rag_pipeline` (–∏–ª–∏ —Å–æ–∑–¥–∞—Ç—å `build_advanced_pipeline`), –¥–æ–±–∞–≤–∏–≤ —Å–ª–µ–¥—É—é—â—É—é –ª–æ–≥–∏–∫—É:\n",
    "1.  **Hybrid Search:** –ï—Å–ª–∏ `cfg.retrieval.mode == 'hybrid'`, —Å–æ–∑–¥–∞–π—Ç–µ `BM25Retriever` –∏ –æ–±—ä–µ–¥–∏–Ω–∏—Ç–µ –µ–≥–æ —Å –≤–µ–∫—Ç–æ—Ä–Ω—ã–º —á–µ—Ä–µ–∑ `QueryFusionRetriever` (–∞–ª–≥–æ—Ä–∏—Ç–º Reciprocal Rank Fusion).\n",
    "2.  **Reranking:** –ï—Å–ª–∏ `cfg.retrieval.use_reranker == True`, –¥–æ–±–∞–≤—å—Ç–µ `SentenceTransformerRerank` –≤ —Å–ø–∏—Å–æ–∫ `node_postprocessors` –¥–ª—è `QueryEngine`. –†–µ—Ä–µ–Ω–∫–µ—Ä –¥–æ–ª–∂–µ–Ω –ø—Ä–∏–Ω–∏–º–∞—Ç—å –Ω–∞ –≤—Ö–æ–¥ —Ç–æ–ø-K –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –∏–∑ —ç—Ç–∞–ø–∞ 1 (`overfetch_k`) –∏ –æ—Å—Ç–∞–≤–ª—è—Ç—å –ª—É—á—à–∏–µ N (`rerank_top_n`).\n",
    "\n",
    "**Concept:** Funnel Architecture (Broad Search -> Narrow Filtering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "KCgRQJfKR-st",
   "metadata": {
    "id": "KCgRQJfKR-st"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grigorii\\AppData\\Local\\Temp\\ipykernel_40064\\3262185644.py:15: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11be45f976bb401bb2e7e80cef07b97f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/5183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd5cda11d13e43d9a6bc3a4287c596c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc11559ca47245968ebb67c09fe689a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf579473f2d14f3eb88e6754bb4b88fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/1650 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8c13d6115d4282b8f45ef0291e51c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retr matryoshka_512:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ LEADERBOARD [RETRIEVAL | MAIN] üèÜ\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_name</th>\n",
       "      <th>ndcg@k</th>\n",
       "      <th>recall@k</th>\n",
       "      <th>ref_free_mean_score</th>\n",
       "      <th>ref_free_redundancy</th>\n",
       "      <th>latency_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hierarchical</td>\n",
       "      <td>0.683837</td>\n",
       "      <td>0.767000</td>\n",
       "      <td>0.565709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.681055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chunk_256</td>\n",
       "      <td>0.678425</td>\n",
       "      <td>0.760750</td>\n",
       "      <td>0.578076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.496207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chunk_1024</td>\n",
       "      <td>0.674990</td>\n",
       "      <td>0.763250</td>\n",
       "      <td>0.572649</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.664925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline</td>\n",
       "      <td>0.674031</td>\n",
       "      <td>0.769500</td>\n",
       "      <td>0.573575</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.452680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>matryoshka_512</td>\n",
       "      <td>0.655511</td>\n",
       "      <td>0.751583</td>\n",
       "      <td>0.605584</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.032776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         run_name    ndcg@k  recall@k  ref_free_mean_score  \\\n",
       "3    hierarchical  0.683837  0.767000             0.565709   \n",
       "1       chunk_256  0.678425  0.760750             0.578076   \n",
       "2      chunk_1024  0.674990  0.763250             0.572649   \n",
       "0        baseline  0.674031  0.769500             0.573575   \n",
       "4  matryoshka_512  0.655511  0.751583             0.605584   \n",
       "\n",
       "   ref_free_redundancy  latency_s  \n",
       "3                  0.0  26.681055  \n",
       "1                  0.0  26.496207  \n",
       "2                  0.0  21.664925  \n",
       "0                  0.0  22.452680  \n",
       "4                  0.0  19.032776  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings have been explicitly disabled. Using MockEmbedding.\n",
      "\n",
      "üèÜ LEADERBOARD [RETRIEVAL | MAIN] üèÜ\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_name</th>\n",
       "      <th>ndcg@k</th>\n",
       "      <th>recall@k</th>\n",
       "      <th>ref_free_mean_score</th>\n",
       "      <th>ref_free_redundancy</th>\n",
       "      <th>latency_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hierarchical</td>\n",
       "      <td>0.683837</td>\n",
       "      <td>0.767000</td>\n",
       "      <td>0.565709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.681055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chunk_256</td>\n",
       "      <td>0.678425</td>\n",
       "      <td>0.760750</td>\n",
       "      <td>0.578076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.496207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chunk_1024</td>\n",
       "      <td>0.674990</td>\n",
       "      <td>0.763250</td>\n",
       "      <td>0.572649</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.664925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline</td>\n",
       "      <td>0.674031</td>\n",
       "      <td>0.769500</td>\n",
       "      <td>0.573575</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.452680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>matryoshka_512</td>\n",
       "      <td>0.655511</td>\n",
       "      <td>0.751583</td>\n",
       "      <td>0.605584</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.032776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         run_name    ndcg@k  recall@k  ref_free_mean_score  \\\n",
       "3    hierarchical  0.683837  0.767000             0.565709   \n",
       "1       chunk_256  0.678425  0.760750             0.578076   \n",
       "2      chunk_1024  0.674990  0.763250             0.572649   \n",
       "0        baseline  0.674031  0.769500             0.573575   \n",
       "4  matryoshka_512  0.655511  0.751583             0.605584   \n",
       "\n",
       "   ref_free_redundancy  latency_s  \n",
       "3                  0.0  26.681055  \n",
       "1                  0.0  26.496207  \n",
       "2                  0.0  21.664925  \n",
       "0                  0.0  22.452680  \n",
       "4                  0.0  19.032776  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Upgrade Pipeline Logic (Hybrid, Reranker)\n",
    "# def build_rag_pipeline(...):\n",
    "#     ...\n",
    "\n",
    "# TODO: Run Experiments\n",
    "# cfg_matryoshka = ...\n",
    "# cfg_hybrid_rerank = ...\n",
    "# run_retrieval(cfg_hybrid_rerank, \"main\", \"advanced_full\")\n",
    "# show_leaderboard(\"retrieval\", \"main\")\n",
    "cfg_matryoshka = RAGConfig(\n",
    "    name=\"matryoshka_512\",\n",
    "    chunking=ChunkingConfig(chunk_size=512, chunk_overlap=50),\n",
    "    retrieval=RetrievalConfig(top_k=5, overfetch_k=20, mode=\"dense\"),\n",
    "    embedding=EmbeddingConfig(model_name=\"Qwen/Qwen3-Embedding-0.6B\", truncate_dim=512),\n",
    "    qdrant_collection=\"scifact_mrl512_chunk512\",\n",
    "    recreate_collection=True,  # –º–µ–Ω—è–µ—Ç—Å—è —ç–º–±–µ–¥–¥–∏–Ω–≥-–≥–µ–æ–º–µ—Ç—Ä–∏—è => –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å\n",
    ")\n",
    "\n",
    "run_retrieval(cfg_matryoshka, \"main\", \"matryoshka_512\")\n",
    "show_leaderboard(\"retrieval\", \"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38708c0b-2116-44e1-9f38-f79c39794f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_hybrid_rerank = RAGConfig(\n",
    "    name=\"hybrid_rerank\",\n",
    "    chunking=ChunkingConfig(chunk_size=512, chunk_overlap=50),\n",
    "    retrieval=RetrievalConfig(\n",
    "        top_k=5,\n",
    "        overfetch_k=40,      # —à–∏—Ä–µ –≤–æ—Ä–æ–Ω–∫–∞\n",
    "        mode=\"hybrid\",\n",
    "        use_reranker=True,\n",
    "        rerank_top_n=8       # –ø–æ—Å–ª–µ —Ä–µ—Ä–∞–Ω–∫–∞ –æ—Å—Ç–∞–≤–ª—è–µ–º N\n",
    "    ),\n",
    "    qdrant_collection=\"scifact_hybrid_512\",\n",
    "    recreate_collection=True,\n",
    "    rerank_model=\"Qwen/Qwen3-Reranker-0.6B\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7e19a6-83e0-4896-932b-fa697c3ae7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_retrieval(cfg_hybrid_rerank, \"main\", \"advanced_full\")\n",
    "show_leaderboard(\"retrieval\", \"main\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "B42YwjKrR-st",
   "metadata": {
    "id": "B42YwjKrR-st"
   },
   "source": [
    "## Task 3: Generator Tuning (Prompt Engineering)\n",
    "\n",
    "–ö–∞—á–µ—Å—Ç–≤–æ —Ä–µ—Ç—Ä–∏–≤–∞–ª–∞ (nDCG) –º—ã –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–ª–∏. –¢–µ–ø–µ—Ä—å —Ñ–æ–∫—É—Å –Ω–∞ –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä–µ.\n",
    "\n",
    "**–ó–∞–¥–∞–Ω–∏–µ:**\n",
    "–ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–π—Ç–µ `prompt_template` –≤ `LLMConfig`. –¶–µ–ª—å ‚Äî –º–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ **Faithfulness** –∏ **Answer Relevancy**.\n",
    "–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏:\n",
    "*   **Role Prompting:** (\"You are an expert scientist...\")\n",
    "*   **Constraint Enforcement:** (\"Answer ONLY based on the context. If unsure, say 'I don't know'.\")\n",
    "*   **Chain-of-Thought:** (\"Let's analyze the context step by step...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "Yvwg2j_PR-st",
   "metadata": {
    "id": "Yvwg2j_PR-st"
   },
   "outputs": [],
   "source": [
    "# TODO: Create Prompt Config\n",
    "# cfg_prompt = ...\n",
    "# run_e2e(cfg_prompt, \"main\", \"prompt_tuned\")\n",
    "# show_leaderboard(\"e2e\", \"main\")\n",
    "\n",
    "prompt_tuned = \"\"\"You are a careful scientific assistant.\n",
    "\n",
    "RULES:\n",
    "- Answer using ONLY the provided context.\n",
    "- If the context does not contain enough information to answer, say: \"I don't know based on the provided context.\"\n",
    "- Be concise and directly address the query.\n",
    "- When possible, quote or paraphrase the exact supporting sentence(s) from the context.\n",
    "\n",
    "Context:\n",
    "{context_str}\n",
    "\n",
    "Query: {query_str}\n",
    "\n",
    "Answer (follow the RULES):\"\"\"\n",
    "\n",
    "cfg_prompt = replace(\n",
    "    cfg_hybrid_rerank,\n",
    "    name=\"prompt_tuned\",\n",
    "    llm=LLMConfig(\n",
    "        model_name=cfg_hybrid_rerank.llm.model_name,\n",
    "        max_new_tokens=256,\n",
    "        context_window=2048,\n",
    "        load_in_4bit=True,\n",
    "        e2e_eval_n=10,\n",
    "        prompt_template=prompt_tuned,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "865e5526-8f0a-4da5-8e0a-278785bb9908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a3517b1a73144dcaf533acf2c2e3dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bbdb28af67541688135ef07e748bc48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf44aa907ab47ce9d3ad7f61d292b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be77571f0cb34b2cb75814bfbe33bb19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Soft\\Anaconda\\Lib\\site-packages\\accelerate\\utils\\modeling.py:804: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  _ = torch.tensor([0], device=i)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45267a4029f841a784180e25f060c2e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YES\n"
     ]
    }
   ],
   "source": [
    "llm = get_llm(cfg_prompt.llm)\n",
    "print(llm.complete(\"Say 'YES' and nothing else.\").text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f20d7f1a-c762-4767-8002-480fad3d8946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grigorii\\AppData\\Local\\Temp\\ipykernel_40136\\2074746868.py:8: UserWarning: Local mode is not recommended for collections with more than 20,000 points. Collection <scifact_hybrid_512> contains 57460 points. Consider using Qdrant in Docker or Qdrant Cloud for better performance with large datasets.\n",
      "  _QDRANT_CLIENT = QdrantClient(path=QDRANT_PATH)\n",
      "C:\\Users\\Grigorii\\AppData\\Local\\Temp\\ipykernel_40136\\2042256602.py:15: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b3866b2a70f411997494d17aea343dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/5183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b4cb551db2f443a82c25eafa21746dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Soft\\Anaconda\\Lib\\site-packages\\qdrant_client\\qdrant_client.py:1868: UserWarning: Local mode is not recommended for collections with more than 20,000 points. Current collection contains 59508 points. Consider using Qdrant in Docker or Qdrant Cloud for better performance with large datasets.\n",
      "  return self._client.upload_points(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee6bf64e13194b6c84066190830dd393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f94b2556f12d4372b53a0e5d89d7689b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/1650 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "affa57f9fc8a40babcfb8c7806f66680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrieving Contexts:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings have been explicitly disabled. Using MockEmbedding.\n",
      "Embeddings have been explicitly disabled. Using MockEmbedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186a51822e64418d8c97d8cbea993844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generations saved to C:\\Users\\Grigorii\\Tasks from HSE\\NLP\\NLP\\RAG\\artifacts\\logs\\generations_prompt_tuned_main.json\n",
      "Embeddings have been explicitly disabled. Using MockEmbedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85e971b62424a6889e91835956e08dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Judging prompt_tuned:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ LEADERBOARD [E2E | MAIN] üèÜ\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_name</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>latency_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>prompt_tuned</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>111.162622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>prompt_tuned</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>341.332121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>prompt_tuned</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>310.618638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       run_name  faithfulness  answer_relevancy   latency_s\n",
       "9  prompt_tuned           0.7               0.7  111.162622\n",
       "6  prompt_tuned           0.0               0.0  341.332121\n",
       "8  prompt_tuned           0.0               0.0  310.618638"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ LEADERBOARD [E2E | MAIN] üèÜ\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_name</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>latency_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>prompt_tuned</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>111.162622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>prompt_tuned</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>341.332121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>prompt_tuned</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>310.618638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       run_name  faithfulness  answer_relevancy   latency_s\n",
       "9  prompt_tuned           0.7               0.7  111.162622\n",
       "6  prompt_tuned           0.0               0.0  341.332121\n",
       "8  prompt_tuned           0.0               0.0  310.618638"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_e2e(cfg_prompt, \"main\", \"prompt_tuned\")\n",
    "show_leaderboard(\"e2e\", \"main\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noGQ5q-JR-su",
   "metadata": {
    "id": "noGQ5q-JR-su"
   },
   "source": [
    "## 4. Final Challenge\n",
    "\n",
    "–í—ã–±–µ—Ä–∏—Ç–µ –æ–¥–Ω—É –ª—É—á—à—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø–æ —Å–æ–≤–æ–∫—É–ø–Ω–æ—Å—Ç–∏ –º–µ—Ç—Ä–∏–∫. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –µ—ë –Ω–∞ –æ—Ç–ª–æ–∂–µ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ **Challenge Split**.\n",
    "–°—Ä–∞–≤–Ω–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å Baseline –Ω–∞ —ç—Ç–æ–º –∂–µ —Å–ø–ª–∏—Ç–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a1cefd4-3aab-4cd5-a505-260d177ef107",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_cfg = RAGConfig(\n",
    "    name=\"baseline\",\n",
    "    chunking=ChunkingConfig(chunk_size=512, chunk_overlap=50),\n",
    "    retrieval=RetrievalConfig(top_k=5, overfetch_k=15),\n",
    "    qdrant_collection=\"scifact_base_512\",\n",
    "    recreate_collection=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "miAvO1DqR-su",
   "metadata": {
    "id": "miAvO1DqR-su"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grigorii\\AppData\\Local\\Temp\\ipykernel_40136\\2042256602.py:15: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c11719d1b8744652889c14a84e2ff601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/5183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ab6b8c79e0849da95bb78150108996b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a80d49886dee48d6b0d7b9702466dee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a17dafa9c15945a6b98bf8ea82185b1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/1650 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a801da6feb44884987ff2d9a606e5f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retr baseline_submission:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ LEADERBOARD [RETRIEVAL | CHALLENGE] üèÜ\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_name</th>\n",
       "      <th>ndcg@k</th>\n",
       "      <th>recall@k</th>\n",
       "      <th>ref_free_mean_score</th>\n",
       "      <th>ref_free_redundancy</th>\n",
       "      <th>latency_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>final_submission</td>\n",
       "      <td>0.762624</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.510316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>490.441892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>baseline_submission</td>\n",
       "      <td>0.636677</td>\n",
       "      <td>0.763333</td>\n",
       "      <td>0.556162</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.838024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               run_name    ndcg@k  recall@k  ref_free_mean_score  \\\n",
       "10     final_submission  0.762624  0.850000             0.510316   \n",
       "12  baseline_submission  0.636677  0.763333             0.556162   \n",
       "\n",
       "    ref_free_redundancy   latency_s  \n",
       "10                  0.0  490.441892  \n",
       "12                  0.0    7.838024  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings have been explicitly disabled. Using MockEmbedding.\n",
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grigorii\\AppData\\Local\\Temp\\ipykernel_40136\\2042256602.py:15: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7236849af204e9a9a330cd8288ab7ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/5183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8449a84e7d3a4c628f445fa29eed305d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59629920b3d6469c94991105aa12dfc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e00b7a91fe6f4036b001c5101d51d08d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/1650 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06d597b5fc24971bbd61bc1d12b71b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrieving Contexts:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings have been explicitly disabled. Using MockEmbedding.\n",
      "Embeddings have been explicitly disabled. Using MockEmbedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b6c64987434d32a02cec9cff620198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generations saved to C:\\Users\\Grigorii\\Tasks from HSE\\NLP\\NLP\\RAG\\artifacts\\logs\\generations_baseline_submission_challenge.json\n",
      "Embeddings have been explicitly disabled. Using MockEmbedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8e26c8222664704b27e589f1240592c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Judging baseline_submission:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ LEADERBOARD [E2E | CHALLENGE] üèÜ\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_name</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>latency_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>final_submission</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>332.567388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>baseline_submission</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>332.301167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               run_name  faithfulness  answer_relevancy   latency_s\n",
       "11     final_submission           0.9               1.0  332.567388\n",
       "13  baseline_submission           0.9               1.0  332.301167"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL RESULTS:\n",
      "\n",
      "üèÜ LEADERBOARD [RETRIEVAL | CHALLENGE] üèÜ\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_name</th>\n",
       "      <th>ndcg@k</th>\n",
       "      <th>recall@k</th>\n",
       "      <th>ref_free_mean_score</th>\n",
       "      <th>ref_free_redundancy</th>\n",
       "      <th>latency_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>final_submission</td>\n",
       "      <td>0.762624</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.510316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>490.441892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>baseline_submission</td>\n",
       "      <td>0.636677</td>\n",
       "      <td>0.763333</td>\n",
       "      <td>0.556162</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.838024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               run_name    ndcg@k  recall@k  ref_free_mean_score  \\\n",
       "10     final_submission  0.762624  0.850000             0.510316   \n",
       "12  baseline_submission  0.636677  0.763333             0.556162   \n",
       "\n",
       "    ref_free_redundancy   latency_s  \n",
       "10                  0.0  490.441892  \n",
       "12                  0.0    7.838024  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ LEADERBOARD [E2E | CHALLENGE] üèÜ\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_name</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>latency_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>final_submission</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>332.567388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>baseline_submission</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>332.301167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               run_name  faithfulness  answer_relevancy   latency_s\n",
       "11     final_submission           0.9               1.0  332.567388\n",
       "13  baseline_submission           0.9               1.0  332.301167"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Final Evaluation\n",
    "# run_retrieval(best_cfg, \"challenge\", \"final_submission\")\n",
    "# run_e2e(best_cfg, \"challenge\", \"final_submission\")\n",
    "\n",
    "run_retrieval(baseline_cfg, \"challenge\", \"baseline_submission\")\n",
    "run_e2e(baseline_cfg, \"challenge\", \"baseline_submission\")\n",
    "\n",
    "print(\"FINAL RESULTS:\")\n",
    "show_leaderboard(\"retrieval\", \"challenge\")\n",
    "show_leaderboard(\"e2e\", \"challenge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6faee2b-b9bf-4be8-8aec-2ec214eb464b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
